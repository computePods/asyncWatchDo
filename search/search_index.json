{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ComputePods Asynchronous Watch-Do tools This tool uses the Python asyncio , asyncinotify and aiofiles libraries to monitor a number of watch-do tasks in \"parallel\". Each watch-do task consists of a number of directories and/or files to be watched for changes. On any change, a corresponding task is run, the output captured and appended to a long running log file associated with that task. Each watch-do task should be provided with a projectDir , a list of watches (directories/files relative to the projectDir ), and a command. You can use standard Python str.format notation to format the command. The command string format will be provided with the dict of configured tasks. Each watch-do task will automatically be provided a workDir as well as a logFile (opened on the path logFilePath ), which will be located in that task's workDir . On Linux, by default a task's workDir will be located in the /tmp directory, and so will be automatically removed on each reboot. Installation This python tool has not (yet) been uploaded to pypi.org . So to install it you need to use: pip install git+https://github.com/computePods/asyncWatchDo/ ( see Examples 5. Install a project from VCS ) or pipx install git+https://github.com/computePods/asyncWatchDo/ ( see installing from source control ). cpawd command The cpawd command looks for its configuration in a cpawdConfig.yaml YAML file located in the directory in which the cpawd is started. The cpawd command takes three optional arguments, --verbose (to report the loaded configuration), --config (to layer on additional configuration files), and --help . dev:~/dev/computePods/asyncWatchDo$ cpawd --help usage: cpawd [-h] [-c CONFIG] [-v | --verbose | --no-verbose] Asynchronously watch multiple directories and perform actions on changes. optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG overlay configuration from file -v, --verbose, --no-verbose provide more detailed output (default: False) Configuration file The cpawdConfig.yaml file expects three sections: tasks : is a dict of watch-do task descriptions, each of which is itself a dict with the keys, cmd , projectDir , and watch . The projectDir key provides a single main directory from which all of the watch paths are expected to be located relatively. The watch key is a list of paths, relative to the projectDir , to be (recursively) watched for changes. The cmd key is a command line which should be run whenever any change is detected. (If no projectDir is provided, it will be assigned to the workDir . The watch paths, can be specified with either a leading ~ or / . Watch paths with leading ~ will be relative to the user's home directory. Watch paths with leading / are assumed to be absolute paths and are not altered.) verbose : is a Boolean which if True , will report the loaded configuration. The default is False . workDir : is a dict with the keys baseDir and prefix . This workDir is used to specify the base workDir for all of the work-do task's individual workDirs . The base workDir will be located in the baseDir and will have the name consisting of the prefix appended with the date and time the cpawd command was started. The default baseDir is /tmp and the default prefix is cpawd . Example An example cpawdConfig.yaml configuration file might be: tasks: webServer: watch: - html cmd: \"livereload {webServer[workDir]}/html\" computePods: projectDir: ~/dev/computePods/computePods.github.io watch: - docs cmd: \"mkdocs --verbose --site-dir {webServer[workDir]}/html\" pythonUtils: projectDir: ~/dev/computePods/pythonUtils watch: - cputils - tests cmd: \"mkdocs --verbose --site-dir {webServer[workDir]}/html/pythonUtils\" interfaces: projectDir: ~/dev/computePods/interfaces watch: - docs - interaces cmd: \"mkdocs --verbose --site-dir {webServer[workDir]}/html/interfaces\" Notes : The {webServer[workDir]} in each of the above cmd keys will be dynamically replaced (using the str.format function) to the value of the webServer watch-do task's workDir . You can add your own keys in each of the tasks. These keys will also be available to the command str.format function invocation. Since we have not provided either of the verbose or workDir sections, they will automatically default to False and /tmp/cpawd-YYYYMMDD-HHMMSS . This example cpawdConfig.yaml file implements a simple multi-repository mkdocs tool, similar to monorepo . However by using cpawd to implement a multi-repository mkdocs tool, the mkdocs invocations in each repository are completely separate from each other. (Alas, when using monorepo to implement multi-repository documentation, the monorepo extension interferes with many of the other mkdocs extensions). Output When run, the cpawd command will out put a list of the configured log files: --------------------------------------------------------------- Logfiles for each task: webServer tail -f /tmp/cpawd-20210724-171805/webServer/command.log computePods tail -f /tmp/cpawd-20210724-171805/computePods/command.log pythonUtils tail -f /tmp/cpawd-20210724-171805/pythonUtils/command.log interfaces tail -f /tmp/cpawd-20210724-171805/interfaces/command.log --------------------------------------------------------------- This list of log files is then followed by a \"stream of consciousness\" list of tasks run. If you copy and paste any of the log file commands (as above) in the 'tab' of a terminal emulator, you will be able to watch the outputs of the respective tasks as they are run.","title":"ComputePods Asynchronous Watch-Do tools"},{"location":"#computepods-asynchronous-watch-do-tools","text":"This tool uses the Python asyncio , asyncinotify and aiofiles libraries to monitor a number of watch-do tasks in \"parallel\". Each watch-do task consists of a number of directories and/or files to be watched for changes. On any change, a corresponding task is run, the output captured and appended to a long running log file associated with that task. Each watch-do task should be provided with a projectDir , a list of watches (directories/files relative to the projectDir ), and a command. You can use standard Python str.format notation to format the command. The command string format will be provided with the dict of configured tasks. Each watch-do task will automatically be provided a workDir as well as a logFile (opened on the path logFilePath ), which will be located in that task's workDir . On Linux, by default a task's workDir will be located in the /tmp directory, and so will be automatically removed on each reboot.","title":"ComputePods Asynchronous Watch-Do tools"},{"location":"#installation","text":"This python tool has not (yet) been uploaded to pypi.org . So to install it you need to use: pip install git+https://github.com/computePods/asyncWatchDo/ ( see Examples 5. Install a project from VCS ) or pipx install git+https://github.com/computePods/asyncWatchDo/ ( see installing from source control ).","title":"Installation"},{"location":"#cpawd-command","text":"The cpawd command looks for its configuration in a cpawdConfig.yaml YAML file located in the directory in which the cpawd is started. The cpawd command takes three optional arguments, --verbose (to report the loaded configuration), --config (to layer on additional configuration files), and --help . dev:~/dev/computePods/asyncWatchDo$ cpawd --help usage: cpawd [-h] [-c CONFIG] [-v | --verbose | --no-verbose] Asynchronously watch multiple directories and perform actions on changes. optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG overlay configuration from file -v, --verbose, --no-verbose provide more detailed output (default: False)","title":"cpawd command"},{"location":"#configuration-file","text":"The cpawdConfig.yaml file expects three sections: tasks : is a dict of watch-do task descriptions, each of which is itself a dict with the keys, cmd , projectDir , and watch . The projectDir key provides a single main directory from which all of the watch paths are expected to be located relatively. The watch key is a list of paths, relative to the projectDir , to be (recursively) watched for changes. The cmd key is a command line which should be run whenever any change is detected. (If no projectDir is provided, it will be assigned to the workDir . The watch paths, can be specified with either a leading ~ or / . Watch paths with leading ~ will be relative to the user's home directory. Watch paths with leading / are assumed to be absolute paths and are not altered.) verbose : is a Boolean which if True , will report the loaded configuration. The default is False . workDir : is a dict with the keys baseDir and prefix . This workDir is used to specify the base workDir for all of the work-do task's individual workDirs . The base workDir will be located in the baseDir and will have the name consisting of the prefix appended with the date and time the cpawd command was started. The default baseDir is /tmp and the default prefix is cpawd .","title":"Configuration file"},{"location":"#example","text":"An example cpawdConfig.yaml configuration file might be: tasks: webServer: watch: - html cmd: \"livereload {webServer[workDir]}/html\" computePods: projectDir: ~/dev/computePods/computePods.github.io watch: - docs cmd: \"mkdocs --verbose --site-dir {webServer[workDir]}/html\" pythonUtils: projectDir: ~/dev/computePods/pythonUtils watch: - cputils - tests cmd: \"mkdocs --verbose --site-dir {webServer[workDir]}/html/pythonUtils\" interfaces: projectDir: ~/dev/computePods/interfaces watch: - docs - interaces cmd: \"mkdocs --verbose --site-dir {webServer[workDir]}/html/interfaces\" Notes : The {webServer[workDir]} in each of the above cmd keys will be dynamically replaced (using the str.format function) to the value of the webServer watch-do task's workDir . You can add your own keys in each of the tasks. These keys will also be available to the command str.format function invocation. Since we have not provided either of the verbose or workDir sections, they will automatically default to False and /tmp/cpawd-YYYYMMDD-HHMMSS . This example cpawdConfig.yaml file implements a simple multi-repository mkdocs tool, similar to monorepo . However by using cpawd to implement a multi-repository mkdocs tool, the mkdocs invocations in each repository are completely separate from each other. (Alas, when using monorepo to implement multi-repository documentation, the monorepo extension interferes with many of the other mkdocs extensions).","title":"Example"},{"location":"#output","text":"When run, the cpawd command will out put a list of the configured log files: --------------------------------------------------------------- Logfiles for each task: webServer tail -f /tmp/cpawd-20210724-171805/webServer/command.log computePods tail -f /tmp/cpawd-20210724-171805/computePods/command.log pythonUtils tail -f /tmp/cpawd-20210724-171805/pythonUtils/command.log interfaces tail -f /tmp/cpawd-20210724-171805/interfaces/command.log --------------------------------------------------------------- This list of log files is then followed by a \"stream of consciousness\" list of tasks run. If you copy and paste any of the log file commands (as above) in the 'tab' of a terminal emulator, you will be able to watch the outputs of the respective tasks as they are run.","title":"Output"},{"location":"API/cpawd/","text":"cpawd.cli Implements the command line interface for the ComputePods Async based Watch-Do tool. cli () Parse the command line arguments, load the configuration, and then run the tasks using the asyncio.run method. Source code in cpawd/cli.py def cli () : \"\"\" Parse the command line arguments, load the configuration, and then run the tasks using the `asyncio.run` method. \"\"\" argparser = argparse . ArgumentParser ( description = \"Asynchronously watch multiple directories and perform actions on changes.\" ) argparser . add_argument ( \"-c\" , \"--config\" , action = 'append' , default = [], help = \"overlay configuration from file\" ) argparser . add_argument ( \"-v\" , \"--verbose\" , default = False , action = argparse . BooleanOptionalAction , help = \"provide more detailed output\" ) cliArgs = argparser . parse_args () config = loadConfig ( cliArgs ) try : asyncio . run ( runTasks ( config )) except KeyboardInterrupt : print ( \" \\n done!\" )","title":"cpawd.cli"},{"location":"API/cpawd/#cpawdcli","text":"Implements the command line interface for the ComputePods Async based Watch-Do tool.","title":"cpawd.cli"},{"location":"API/cpawd/#cpawd.cli.cli","text":"Parse the command line arguments, load the configuration, and then run the tasks using the asyncio.run method. Source code in cpawd/cli.py def cli () : \"\"\" Parse the command line arguments, load the configuration, and then run the tasks using the `asyncio.run` method. \"\"\" argparser = argparse . ArgumentParser ( description = \"Asynchronously watch multiple directories and perform actions on changes.\" ) argparser . add_argument ( \"-c\" , \"--config\" , action = 'append' , default = [], help = \"overlay configuration from file\" ) argparser . add_argument ( \"-v\" , \"--verbose\" , default = False , action = argparse . BooleanOptionalAction , help = \"provide more detailed output\" ) cliArgs = argparser . parse_args () config = loadConfig ( cliArgs ) try : asyncio . run ( runTasks ( config )) except KeyboardInterrupt : print ( \" \\n done!\" )","title":"cli()"},{"location":"API/fsWatcher/","text":"cpawd.fsWatcher This cpawd.fsWatcher module adapts the asyncinotify example to recursively watch directories or files either by a direct request, or as they are created inside watched directories. FSWatcher The FSWatcher class manages the Linux file system inotify watches for a given collection of directories or files. It provides a file change event stream via the iterable recursive_watch method. To allow for asynchronous operation, the \"watches\" are added to an asyncio.Queue managed by the managePathsToWatchQueue method. When used, this managePathsToWatchQueue method should be run inside its own asyncio.Task . get_directories_recursive ( self , path ) Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. Source code in cpawd/fsWatcher.py def get_directories_recursive ( self , path ) : ''' Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. ''' if path . is_dir () : yield path for child in path . iterdir (): yield from self . get_directories_recursive ( child ) elif path . is_file () : yield path managePathsToWatchQueue ( self ) async Implement all (pending) requests to watch a directory or file which are in the pathsToWatchQueue . The paths contained in all directories are themselves recursively added to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def managePathsToWatchQueue ( self ) : \"\"\" Implement all (pending) requests to watch a directory or file which are in the `pathsToWatchQueue`. The paths contained in all directories are themselves recursively added to the `pathsToWatchQueue`. \"\"\" while True : aPathToWatch = await self . pathsToWatchQueue . get () for aPath in self . get_directories_recursive ( Path ( aPathToWatch )) : try : self . inotify . add_watch ( aPath , self . wrMask ) self . logger . info ( f 'INIT: watching { aPath } ' ) except PermissionError as err : pass except Exception as err : print ( f \"Exception whild trying to watch: [ { aPath } ]\" ) traceback . print_exc ( err ) # we can't watch this path just yet... # ... schedule its parent and try again... await self . watchAPath ( aPath . parent ) self . pathsToWatchQueue . task_done () watchAPath ( self , pathToWatch ) async Add a single directory or file to be watched by this instance of FSWatcher to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def watchAPath ( self , pathToWatch ) : \"\"\" Add a single directory or file to be watched by this instance of `FSWatcher` to the `pathsToWatchQueue`. \"\"\" await self . pathsToWatchQueue . put ( pathToWatch ) watchForFileSystemEvents ( self ) An asynchronously interable method which yields file system change events. Source code in cpawd/fsWatcher.py async def watchForFileSystemEvents ( self ): \"\"\" An asynchronously interable method which yields file system change events. \"\"\" # Things that can throw this off: # # * Moving a watched directory out of the watch tree (will still # generate events even when outside of directory tree) # # * Doing two changes on a directory or something before the program # has a time to handle it (this will also throw off a lot of inotify # code, though) # # * Moving a watched directory within a watched directory will get the # wrong path. This needs to use the cookie system to link events # together and complete the move properly, which can still make some # events get the wrong path if you get file events during the move or # something silly like that, since MOVED_FROM and MOVED_TO aren't # guaranteed to be contiguous. That exercise is left up to the # reader. # # * Trying to watch a path that doesn't exist won't automatically # create it or anything of the sort. # # * Deleting and recreating or moving the watched directory won't do # anything special, but it probably should. # async for event in self . inotify : # If this is a creation event, add a watch for the new path (and its # subdirectories if any) # if Mask . CREATE in event . mask and event . path is not None : await self . watchAPath ( event . path ) # If there are some bits in the cpMask in the event.mask yield this # event # if event . mask & self . cpMask : yield event else : # Note that these events are needed for cleanup purposes. # We'll always get IGNORED events so the watch can be removed # from the inotify. We don't need to do anything with the # events, but they do need to be generated for cleanup. # We don't need to pass IGNORED events up, because the end-user # doesn't have the inotify instance anyway, and IGNORED is just # used for management purposes. # self . logger . debug ( f 'UNYIELDED EVENT: { event } ' )","title":"cpawd.fsWatcher"},{"location":"API/fsWatcher/#cpawdfswatcher","text":"This cpawd.fsWatcher module adapts the asyncinotify example to recursively watch directories or files either by a direct request, or as they are created inside watched directories.","title":"cpawd.fsWatcher"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher","text":"The FSWatcher class manages the Linux file system inotify watches for a given collection of directories or files. It provides a file change event stream via the iterable recursive_watch method. To allow for asynchronous operation, the \"watches\" are added to an asyncio.Queue managed by the managePathsToWatchQueue method. When used, this managePathsToWatchQueue method should be run inside its own asyncio.Task .","title":"FSWatcher"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.get_directories_recursive","text":"Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. Source code in cpawd/fsWatcher.py def get_directories_recursive ( self , path ) : ''' Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. ''' if path . is_dir () : yield path for child in path . iterdir (): yield from self . get_directories_recursive ( child ) elif path . is_file () : yield path","title":"get_directories_recursive()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.managePathsToWatchQueue","text":"Implement all (pending) requests to watch a directory or file which are in the pathsToWatchQueue . The paths contained in all directories are themselves recursively added to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def managePathsToWatchQueue ( self ) : \"\"\" Implement all (pending) requests to watch a directory or file which are in the `pathsToWatchQueue`. The paths contained in all directories are themselves recursively added to the `pathsToWatchQueue`. \"\"\" while True : aPathToWatch = await self . pathsToWatchQueue . get () for aPath in self . get_directories_recursive ( Path ( aPathToWatch )) : try : self . inotify . add_watch ( aPath , self . wrMask ) self . logger . info ( f 'INIT: watching { aPath } ' ) except PermissionError as err : pass except Exception as err : print ( f \"Exception whild trying to watch: [ { aPath } ]\" ) traceback . print_exc ( err ) # we can't watch this path just yet... # ... schedule its parent and try again... await self . watchAPath ( aPath . parent ) self . pathsToWatchQueue . task_done ()","title":"managePathsToWatchQueue()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.watchAPath","text":"Add a single directory or file to be watched by this instance of FSWatcher to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def watchAPath ( self , pathToWatch ) : \"\"\" Add a single directory or file to be watched by this instance of `FSWatcher` to the `pathsToWatchQueue`. \"\"\" await self . pathsToWatchQueue . put ( pathToWatch )","title":"watchAPath()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.watchForFileSystemEvents","text":"An asynchronously interable method which yields file system change events. Source code in cpawd/fsWatcher.py async def watchForFileSystemEvents ( self ): \"\"\" An asynchronously interable method which yields file system change events. \"\"\" # Things that can throw this off: # # * Moving a watched directory out of the watch tree (will still # generate events even when outside of directory tree) # # * Doing two changes on a directory or something before the program # has a time to handle it (this will also throw off a lot of inotify # code, though) # # * Moving a watched directory within a watched directory will get the # wrong path. This needs to use the cookie system to link events # together and complete the move properly, which can still make some # events get the wrong path if you get file events during the move or # something silly like that, since MOVED_FROM and MOVED_TO aren't # guaranteed to be contiguous. That exercise is left up to the # reader. # # * Trying to watch a path that doesn't exist won't automatically # create it or anything of the sort. # # * Deleting and recreating or moving the watched directory won't do # anything special, but it probably should. # async for event in self . inotify : # If this is a creation event, add a watch for the new path (and its # subdirectories if any) # if Mask . CREATE in event . mask and event . path is not None : await self . watchAPath ( event . path ) # If there are some bits in the cpMask in the event.mask yield this # event # if event . mask & self . cpMask : yield event else : # Note that these events are needed for cleanup purposes. # We'll always get IGNORED events so the watch can be removed # from the inotify. We don't need to do anything with the # events, but they do need to be generated for cleanup. # We don't need to pass IGNORED events up, because the end-user # doesn't have the inotify instance anyway, and IGNORED is just # used for management purposes. # self . logger . debug ( f 'UNYIELDED EVENT: { event } ' )","title":"watchForFileSystemEvents()"},{"location":"API/loadConfiguration/","text":"cpawd.loadConfiguration Load and normalise the cpawd configuration loadConfig ( cliArgs ) Load the configuration by merging any cpawdConfig.yaml found in the current working directory, and then any other configuration files specified on the command line. Then perform the following normalisation: The base working directory is computed using the baseDir and prefix keys found in the workDir section of the merged configuration. Compute workDir for each task in the tasks section of the merged configuration. Ensure all workDir exists (both for the base and the individual tasks) Expand all watched paths to an absolute path in the file system. Check that the projectDir exists for each task. Compute logFilePaths and open logFiles for each task. Source code in cpawd/loadConfiguration.py def loadConfig ( cliArgs ) : \"\"\" Load the configuration by merging any `cpawdConfig.yaml` found in the current working directory, and then any other configuration files specified on the command line. Then perform the following normalisation: - The base working directory is computed using the `baseDir` and `prefix` keys found in the `workDir` section of the merged configuration. - Compute `workDir` for each task in the `tasks` section of the merged configuration. - Ensure all `workDir` exists (both for the base and the individual tasks) - Expand all watched paths to an absolute path in the file system. - Check that the `projectDir` exists for each task. - Compute logFilePaths and open logFiles for each task. \"\"\" config = { 'workDir' : { 'baseDir' : '/tmp' , 'prefix' : 'cpawd' }, 'tasks' : {}, 'verbose' : False } if cliArgs . verbose : config [ 'verbose' ] = cliArgs . verbose cliArgs . config . insert ( 0 , 'cpawdConfig.yaml' ) for aConfigPath in cliArgs . config : if os . path . exists ( aConfigPath ) : try : with open ( aConfigPath ) as aConfigFile : aConfig = yaml . safe_load ( aConfigFile . read ()) mergeYamlData ( config , aConfig , \"\" ) except Exception as err : print ( \"Could not load configuration from [ {} ]\" . format ( aConfigPath )) print ( err ) # create the working directory if 'workDir' not in config [ 'workDir' ] : config [ 'workDir' ][ 'workDir' ] = os . path . join ( config [ 'workDir' ][ 'baseDir' ], config [ 'workDir' ][ 'prefix' ] + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ) ) workDir = config [ 'workDir' ][ 'workDir' ] if os . path . exists ( workDir ) : shutil . rmtree ( workDir ) os . makedirs ( workDir ) # ensure the task work and project directories exist for aTaskName , aTask in config [ 'tasks' ] . items () : aTask [ 'workDir' ] = os . path . join ( workDir , aTaskName ) os . makedirs ( aTask [ 'workDir' ]) aTask [ 'logFilePath' ] = os . path . join ( workDir , aTaskName , 'command.log' ) if 'projectDir' in aTask : aTask [ 'projectDir' ] = os . path . abspath ( os . path . expanduser ( aTask [ 'projectDir' ])) else : aTask [ 'projectDir' ] = aTask [ 'workDir' ] if not os . path . exists ( aTask [ 'projectDir' ]) : print ( \"ERROR: the projectDir for task {} MUST exist in the file system\" . format ( aTaskName )) print ( \"---------------------------------------------------------\" ) print ( yaml . dump ( aTask )) print ( \"---------------------------------------------------------\" ) sys . exit ( - 1 ) if 'watch' not in aTask or not aTask [ 'watch' ] : print ( \"ERROR: all tasks MUST have a collection of files/directories to watch\" ) print ( \" no 'watch' list provided in task [ {} ]:\" . format ( aTaskName )) print ( \"---------------------------------------------------------\" ) print ( yaml . dump ( aTask )) print ( \"---------------------------------------------------------\" ) sys . exit ( - 1 ) expandedWatches = [] for aWatch in aTask [ 'watch' ] : newWatch = os . path . expanduser ( aWatch ) if not newWatch . startswith ( '/' ) : newWatch = os . path . join ( aTask [ 'projectDir' ], newWatch ) expandedWatches . append ( newWatch ) aTask [ 'watch' ] = expandedWatches # expand commands and open logFiles for aTaskName , aTask in config [ 'tasks' ] . items () : try : aTask [ 'cmd' ] = aTask [ 'cmd' ] . format ( ** config [ 'tasks' ]) except Exception as err : print ( \"Could not expand variables in cmd string:\" ) print ( aTask [ 'cmd' ]) print ( repr ( err )) if config [ 'verbose' ] : print ( \"configuration:\" ) print ( \"---------------------------------------------------------------\" ) print ( yaml . dump ( config )) print ( \"---------------------------------------------------------------\" ) # open log files print ( \" \\n Logfiles for each task:\" ) for aTaskName , aTask in config [ 'tasks' ] . items () : aTask [ 'logFile' ] = open ( aTask [ 'logFilePath' ], 'w' ) print ( \" {} \\n tail -f {} \" . format ( aTaskName , aTask [ 'logFilePath' ])) print ( \"\" ) print ( \"---------------------------------------------------------------\" ) print ( \"\" ) return config mergeYamlData ( yamlData , newYamlData , thePath ) This is a generic Python merge. It is a deep merge and handles both dictionaries and arrays Source code in cpawd/loadConfiguration.py def mergeYamlData ( yamlData , newYamlData , thePath ) : \"\"\" This is a generic Python merge. It is a *deep* merge and handles both dictionaries and arrays \"\"\" if type ( yamlData ) is None : print ( \"ERROR yamlData should NEVER be None \" ) sys . exit ( - 1 ) if type ( yamlData ) != type ( newYamlData ) : print ( \"Incompatible types {} and {} while trying to merge YAML data at {} \" . format ( type ( yamlData ), type ( newYamlData ), thePath )) print ( \"Stoping merge at {} \" . format ( thePath )) return if type ( yamlData ) is dict : for key , value in newYamlData . items () : if key not in yamlData : yamlData [ key ] = value elif type ( yamlData [ key ]) is dict : mergeYamlData ( yamlData [ key ], value , thePath + '.' + key ) elif type ( yamlData [ key ]) is list : for aValue in value : yamlData [ key ] . append ( aValue ) else : yamlData [ key ] = value elif type ( yamlData ) is list : for value in newYamlData : yamlData . append ( value ) else : print ( \"ERROR yamlData MUST be either a dictionary or an array.\" ) sys . exit ( - 1 )","title":"cpawd.loadConfiguration"},{"location":"API/loadConfiguration/#cpawdloadconfiguration","text":"Load and normalise the cpawd configuration","title":"cpawd.loadConfiguration"},{"location":"API/loadConfiguration/#cpawd.loadConfiguration.loadConfig","text":"Load the configuration by merging any cpawdConfig.yaml found in the current working directory, and then any other configuration files specified on the command line. Then perform the following normalisation: The base working directory is computed using the baseDir and prefix keys found in the workDir section of the merged configuration. Compute workDir for each task in the tasks section of the merged configuration. Ensure all workDir exists (both for the base and the individual tasks) Expand all watched paths to an absolute path in the file system. Check that the projectDir exists for each task. Compute logFilePaths and open logFiles for each task. Source code in cpawd/loadConfiguration.py def loadConfig ( cliArgs ) : \"\"\" Load the configuration by merging any `cpawdConfig.yaml` found in the current working directory, and then any other configuration files specified on the command line. Then perform the following normalisation: - The base working directory is computed using the `baseDir` and `prefix` keys found in the `workDir` section of the merged configuration. - Compute `workDir` for each task in the `tasks` section of the merged configuration. - Ensure all `workDir` exists (both for the base and the individual tasks) - Expand all watched paths to an absolute path in the file system. - Check that the `projectDir` exists for each task. - Compute logFilePaths and open logFiles for each task. \"\"\" config = { 'workDir' : { 'baseDir' : '/tmp' , 'prefix' : 'cpawd' }, 'tasks' : {}, 'verbose' : False } if cliArgs . verbose : config [ 'verbose' ] = cliArgs . verbose cliArgs . config . insert ( 0 , 'cpawdConfig.yaml' ) for aConfigPath in cliArgs . config : if os . path . exists ( aConfigPath ) : try : with open ( aConfigPath ) as aConfigFile : aConfig = yaml . safe_load ( aConfigFile . read ()) mergeYamlData ( config , aConfig , \"\" ) except Exception as err : print ( \"Could not load configuration from [ {} ]\" . format ( aConfigPath )) print ( err ) # create the working directory if 'workDir' not in config [ 'workDir' ] : config [ 'workDir' ][ 'workDir' ] = os . path . join ( config [ 'workDir' ][ 'baseDir' ], config [ 'workDir' ][ 'prefix' ] + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ) ) workDir = config [ 'workDir' ][ 'workDir' ] if os . path . exists ( workDir ) : shutil . rmtree ( workDir ) os . makedirs ( workDir ) # ensure the task work and project directories exist for aTaskName , aTask in config [ 'tasks' ] . items () : aTask [ 'workDir' ] = os . path . join ( workDir , aTaskName ) os . makedirs ( aTask [ 'workDir' ]) aTask [ 'logFilePath' ] = os . path . join ( workDir , aTaskName , 'command.log' ) if 'projectDir' in aTask : aTask [ 'projectDir' ] = os . path . abspath ( os . path . expanduser ( aTask [ 'projectDir' ])) else : aTask [ 'projectDir' ] = aTask [ 'workDir' ] if not os . path . exists ( aTask [ 'projectDir' ]) : print ( \"ERROR: the projectDir for task {} MUST exist in the file system\" . format ( aTaskName )) print ( \"---------------------------------------------------------\" ) print ( yaml . dump ( aTask )) print ( \"---------------------------------------------------------\" ) sys . exit ( - 1 ) if 'watch' not in aTask or not aTask [ 'watch' ] : print ( \"ERROR: all tasks MUST have a collection of files/directories to watch\" ) print ( \" no 'watch' list provided in task [ {} ]:\" . format ( aTaskName )) print ( \"---------------------------------------------------------\" ) print ( yaml . dump ( aTask )) print ( \"---------------------------------------------------------\" ) sys . exit ( - 1 ) expandedWatches = [] for aWatch in aTask [ 'watch' ] : newWatch = os . path . expanduser ( aWatch ) if not newWatch . startswith ( '/' ) : newWatch = os . path . join ( aTask [ 'projectDir' ], newWatch ) expandedWatches . append ( newWatch ) aTask [ 'watch' ] = expandedWatches # expand commands and open logFiles for aTaskName , aTask in config [ 'tasks' ] . items () : try : aTask [ 'cmd' ] = aTask [ 'cmd' ] . format ( ** config [ 'tasks' ]) except Exception as err : print ( \"Could not expand variables in cmd string:\" ) print ( aTask [ 'cmd' ]) print ( repr ( err )) if config [ 'verbose' ] : print ( \"configuration:\" ) print ( \"---------------------------------------------------------------\" ) print ( yaml . dump ( config )) print ( \"---------------------------------------------------------------\" ) # open log files print ( \" \\n Logfiles for each task:\" ) for aTaskName , aTask in config [ 'tasks' ] . items () : aTask [ 'logFile' ] = open ( aTask [ 'logFilePath' ], 'w' ) print ( \" {} \\n tail -f {} \" . format ( aTaskName , aTask [ 'logFilePath' ])) print ( \"\" ) print ( \"---------------------------------------------------------------\" ) print ( \"\" ) return config","title":"loadConfig()"},{"location":"API/loadConfiguration/#cpawd.loadConfiguration.mergeYamlData","text":"This is a generic Python merge. It is a deep merge and handles both dictionaries and arrays Source code in cpawd/loadConfiguration.py def mergeYamlData ( yamlData , newYamlData , thePath ) : \"\"\" This is a generic Python merge. It is a *deep* merge and handles both dictionaries and arrays \"\"\" if type ( yamlData ) is None : print ( \"ERROR yamlData should NEVER be None \" ) sys . exit ( - 1 ) if type ( yamlData ) != type ( newYamlData ) : print ( \"Incompatible types {} and {} while trying to merge YAML data at {} \" . format ( type ( yamlData ), type ( newYamlData ), thePath )) print ( \"Stoping merge at {} \" . format ( thePath )) return if type ( yamlData ) is dict : for key , value in newYamlData . items () : if key not in yamlData : yamlData [ key ] = value elif type ( yamlData [ key ]) is dict : mergeYamlData ( yamlData [ key ], value , thePath + '.' + key ) elif type ( yamlData [ key ]) is list : for aValue in value : yamlData [ key ] . append ( aValue ) else : yamlData [ key ] = value elif type ( yamlData ) is list : for value in newYamlData : yamlData . append ( value ) else : print ( \"ERROR yamlData MUST be either a dictionary or an array.\" ) sys . exit ( - 1 )","title":"mergeYamlData()"},{"location":"API/taskRunner/","text":"cpawd.taskRunner This cpawd.taskRunner module implements the running of all watch-do tasks. It uses the DebouncingTimer to actually run the tasks after a short timeout period. We use this short timeout to ensure the task is only run once for any collection of changes detected at nearly the same time. The top level runTasks method initiates the asyncio.Tasks which represent each watch-do task. DebouncingTimer The DebouncingTimer class implements a simple timer to ensure multiple file system events result in only one invocation of the task command. __init__ ( self , timeout , taskName , taskDetails ) special Create the timer with a specific timeout and task definition. The taskDetails provides the command to run, the log file used to record command output, as well as the project directory in which to run the command. Source code in cpawd/taskRunner.py def __init__ ( self , timeout , taskName , taskDetails ) : \"\"\" Create the timer with a specific timeout and task definition. The taskDetails provides the command to run, the log file used to record command output, as well as the project directory in which to run the command. \"\"\" self . timeout = timeout self . taskName = taskName self . taskCmd = taskDetails [ 'cmd' ] self . taskLog = taskDetails [ 'logFile' ] self . taskDir = taskDetails [ 'projectDir' ] self . taskFuture = None reStart ( self ) (Re)Start the timer. If the timer is already started, it is restarted with a new timeout period. Source code in cpawd/taskRunner.py def reStart ( self ) : \"\"\" (Re)Start the timer. If the timer is already started, it is restarted with a new timeout period. \"\"\" if self . taskFuture : self . taskFuture . cancel () self . taskFuture = asyncio . ensure_future ( self . taskRunner ()) taskRunner ( self ) async Run the task's command, after sleeping for the timeout period, using asyncio.create_subprocess_shell command. Source code in cpawd/taskRunner.py async def taskRunner ( self ) : \"\"\" Run the task's command, after sleeping for the timeout period, using `asyncio.create_subprocess_shell` command. \"\"\" await asyncio . sleep ( self . timeout ) proc = await asyncio . create_subprocess_shell ( self . taskCmd , stdout = asyncio . subprocess . PIPE , stderr = asyncio . subprocess . STDOUT , cwd = self . taskDir ) stdout , stderr = await proc . communicate () print ( f 'Ran: { self . taskName } ' ) if stdout : self . taskLog . write ( \" \\n ============================================================================ \\n \" ) self . taskLog . write ( \" {} stdout @ {} \\n \" . format ( self . taskName , time . strftime ( \"%Y/%m/ %d %H:%M:%S\" ))) self . taskLog . write ( \" {} \\n \" . format ( self . taskCmd )) self . taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) self . taskLog . write ( stdout . decode ()) self . taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) self . taskLog . flush () if stderr : self . taskLog . write ( \" \\n ============================================================================ \\n \" ) self . taskLog . write ( \" {} stderr @ {} \\n \" . format ( self . taskName , time . strftime ( \"%Y/%m/ %d %H:%M:%S\" ))) self . taskLog . write ( \" {} \\n \" . format ( self . taskCmd )) self . taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) self . taskLog . write ( stderr . decode ()) self . taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) self . taskLog . flush () runTasks ( config ) async Walk through the list of watch-do tasks and create an asyncio.Task for each task, using an invocation of the watchDo method to wrap each task. Since these tasks are not Python-CPU bound, they will essentially \"run\" in parallel. Source code in cpawd/taskRunner.py async def runTasks ( config ) : \"\"\" Walk through the list of watch-do tasks and create an `asyncio.Task` for each task, using an invocation of the `watchDo` method to wrap each task. Since these tasks are not Python-CPU bound, they will essentially \"run\" in parallel. \"\"\" cpawdFuture = asyncio . get_event_loop () . create_future () for aTaskName , aTask in config [ 'tasks' ] . items () : await asyncio . create_task ( watchDo ( aTaskName , aTask )) await cpawdFuture watchDo ( aTaskName , aTask ) async Setup and manage the watches, and then run the task's command using the DebouncingTimer whenever a change is detected in a watched directory or file. Source code in cpawd/taskRunner.py async def watchDo ( aTaskName , aTask ) : \"\"\" Setup and manage the watches, and then run the task's command using the DebouncingTimer whenever a change is detected in a watched directory or file. \"\"\" aWatcher = FSWatcher () aTimer = DebouncingTimer ( 1 , aTaskName , aTask ) # add watches asyncio . create_task ( aWatcher . managePathsToWatchQueue ()) for aWatch in aTask [ 'watch' ] : await aWatcher . watchAPath ( aWatch ) # watch and run cmd async for event in aWatcher . watchForFileSystemEvents () : aTimer . reStart ()","title":"cpawd.taskRunner"},{"location":"API/taskRunner/#cpawdtaskrunner","text":"This cpawd.taskRunner module implements the running of all watch-do tasks. It uses the DebouncingTimer to actually run the tasks after a short timeout period. We use this short timeout to ensure the task is only run once for any collection of changes detected at nearly the same time. The top level runTasks method initiates the asyncio.Tasks which represent each watch-do task.","title":"cpawd.taskRunner"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer","text":"The DebouncingTimer class implements a simple timer to ensure multiple file system events result in only one invocation of the task command.","title":"DebouncingTimer"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.__init__","text":"Create the timer with a specific timeout and task definition. The taskDetails provides the command to run, the log file used to record command output, as well as the project directory in which to run the command. Source code in cpawd/taskRunner.py def __init__ ( self , timeout , taskName , taskDetails ) : \"\"\" Create the timer with a specific timeout and task definition. The taskDetails provides the command to run, the log file used to record command output, as well as the project directory in which to run the command. \"\"\" self . timeout = timeout self . taskName = taskName self . taskCmd = taskDetails [ 'cmd' ] self . taskLog = taskDetails [ 'logFile' ] self . taskDir = taskDetails [ 'projectDir' ] self . taskFuture = None","title":"__init__()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.reStart","text":"(Re)Start the timer. If the timer is already started, it is restarted with a new timeout period. Source code in cpawd/taskRunner.py def reStart ( self ) : \"\"\" (Re)Start the timer. If the timer is already started, it is restarted with a new timeout period. \"\"\" if self . taskFuture : self . taskFuture . cancel () self . taskFuture = asyncio . ensure_future ( self . taskRunner ())","title":"reStart()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.taskRunner","text":"Run the task's command, after sleeping for the timeout period, using asyncio.create_subprocess_shell command. Source code in cpawd/taskRunner.py async def taskRunner ( self ) : \"\"\" Run the task's command, after sleeping for the timeout period, using `asyncio.create_subprocess_shell` command. \"\"\" await asyncio . sleep ( self . timeout ) proc = await asyncio . create_subprocess_shell ( self . taskCmd , stdout = asyncio . subprocess . PIPE , stderr = asyncio . subprocess . STDOUT , cwd = self . taskDir ) stdout , stderr = await proc . communicate () print ( f 'Ran: { self . taskName } ' ) if stdout : self . taskLog . write ( \" \\n ============================================================================ \\n \" ) self . taskLog . write ( \" {} stdout @ {} \\n \" . format ( self . taskName , time . strftime ( \"%Y/%m/ %d %H:%M:%S\" ))) self . taskLog . write ( \" {} \\n \" . format ( self . taskCmd )) self . taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) self . taskLog . write ( stdout . decode ()) self . taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) self . taskLog . flush () if stderr : self . taskLog . write ( \" \\n ============================================================================ \\n \" ) self . taskLog . write ( \" {} stderr @ {} \\n \" . format ( self . taskName , time . strftime ( \"%Y/%m/ %d %H:%M:%S\" ))) self . taskLog . write ( \" {} \\n \" . format ( self . taskCmd )) self . taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) self . taskLog . write ( stderr . decode ()) self . taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) self . taskLog . flush ()","title":"taskRunner()"},{"location":"API/taskRunner/#cpawd.taskRunner.runTasks","text":"Walk through the list of watch-do tasks and create an asyncio.Task for each task, using an invocation of the watchDo method to wrap each task. Since these tasks are not Python-CPU bound, they will essentially \"run\" in parallel. Source code in cpawd/taskRunner.py async def runTasks ( config ) : \"\"\" Walk through the list of watch-do tasks and create an `asyncio.Task` for each task, using an invocation of the `watchDo` method to wrap each task. Since these tasks are not Python-CPU bound, they will essentially \"run\" in parallel. \"\"\" cpawdFuture = asyncio . get_event_loop () . create_future () for aTaskName , aTask in config [ 'tasks' ] . items () : await asyncio . create_task ( watchDo ( aTaskName , aTask )) await cpawdFuture","title":"runTasks()"},{"location":"API/taskRunner/#cpawd.taskRunner.watchDo","text":"Setup and manage the watches, and then run the task's command using the DebouncingTimer whenever a change is detected in a watched directory or file. Source code in cpawd/taskRunner.py async def watchDo ( aTaskName , aTask ) : \"\"\" Setup and manage the watches, and then run the task's command using the DebouncingTimer whenever a change is detected in a watched directory or file. \"\"\" aWatcher = FSWatcher () aTimer = DebouncingTimer ( 1 , aTaskName , aTask ) # add watches asyncio . create_task ( aWatcher . managePathsToWatchQueue ()) for aWatch in aTask [ 'watch' ] : await aWatcher . watchAPath ( aWatch ) # watch and run cmd async for event in aWatcher . watchForFileSystemEvents () : aTimer . reStart ()","title":"watchDo()"}]}