{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ComputePods Asynchronous Watch-Do tools This tool uses the Python asyncio , asyncinotify and aiofiles libraries to monitor a number of watch-do tasks in \"parallel\". Each watch-do task consists of a number of directories and/or files to be watched for changes. On any change, a corresponding task is run, the output captured and appended to a long running log file associated with that task. Each watch-do task should be provided with a projectDir , a list of watches (directories/files relative to the projectDir ), and a command. You can use standard Python str.format notation to format the command. The command string format will be provided with the dict of configured tasks. Each watch-do task will automatically be provided a workDir as well as a logFile (opened on the path logFilePath ), which will be located in that task's workDir . On Linux, by default a task's workDir will be located in the /tmp directory, and so will be automatically removed on each reboot. Installation This python tool has not (yet) been uploaded to pypi.org . So to install it you need to use: pip install git+https://github.com/computePods/asyncWatchDo/ ( see Examples 5. Install a project from VCS ) or pipx install git+https://github.com/computePods/asyncWatchDo/ ( see installing from source control ). cpawd command The cpawd command looks for its configuration in a cpawdConfig.yaml YAML file located in the directory in which the cpawd is started. The cpawd command takes three optional arguments, --verbose (to report the loaded configuration), --config (to layer on additional configuration files), and --help . dev:~/dev/computePods/asyncWatchDo$ cpawd --help usage: cpawd [-h] [-c CONFIG] [-v | --verbose | --no-verbose] [-d | --debug | --no-debug] [-p PAGER] Asynchronously watch multiple directories and perform actions on changes. optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG overlay configuration from file -v, --verbose, --no-verbose show the loaded configuration (default: False) -d, --debug, --no-debug provide debugging output (default: False) -p PAGER, --pager PAGER pager to use in task list (default=moar) Configuration file The cpawdConfig.yaml file expects three sections: tasks : is a dict of watch-do task descriptions, each of which is itself a dict with the keys: cmd : is a list of command line arguments which will be run (using exec) whenever any change is detected. Arguments can use the python str.format to dynamically add information from the tasks dictionary. You can use this to ensure commands for one task, can use information, such as workDir , from any other configured task (see example below). projectDir : provides a single main directory from which all of the watch paths are expected to be located relatively. If no projectDir is provided, it will be assigned to the workDir . watch : is a list of paths, relative to the projectDir, to be (recursively) watched for changes. The watch paths, can, optionally, be specified with either a leading ~ or / . Watch paths with leading ~ will be relative to the user's home directory. Watch paths with leading / are assumed to be absolute paths and are not altered. Watch paths with no leading ~ or / are assumed to be relative to the projectDir. runOnce : is a key which, if it exists, ensures the command is only run once (and is never restarted on file system events). You can use this for any command which provides its own file system watch abilities. toolTips : is a string which will be echoed to the user. You can use this, for example, to inform the user of the configured URL provided by a web-server task. All other keys will be provided to the str.format when expanding either the command arguments or the toolTips (see the example below). verbose : is a Boolean which if True , will report the loaded configuration. The default is False . workDir : is a dict with the keys baseDir and prefix . This workDir is used to specify the base workDir for all of the work-do task's individual workDirs . The base workDir will be located in the baseDir and will have the name consisting of the prefix appended with the date and time the cpawd command was started. The default baseDir is /tmp and the default prefix is cpawd . Example An example cpawdConfig.yaml configuration file might be: tasks: webServer: runOnce: true toolTips: \"http://localhost:{webServer[port]}\" port: \"8008\" watch: - html cmd: - cphttp - -v - -l - debug - p - \"{webServer[port]}\" - -d - \"{webServer[workDir]}/html\" - -w - \"{webServer[workDir]}/html\" computePods: projectDir: ~/dev/computePods/computePods.github.io watch: - docs cmd: - mkdocs - --verbose - --site-dir - \"{webServer[workDir]}/html\" pythonUtils: projectDir: ~/dev/computePods/pythonUtils watch: - cputils - tests cmd: - mkdocs - --verbose - --site-dir - \"{webServer[workDir]}/html/pythonUtils\" interfaces: projectDir: ~/dev/computePods/interfaces watch: - docs - interaces cmd: - mkdocs - --verbose - --site-dir - \"{webServer[workDir]}/html/interfaces\" Notes : The {webServer[workDir]} in each of the above cmd keys will be dynamically replaced (using the str.format function) to the value of the webServer watch-do task's workDir . You can add your own keys in each of the tasks (for example the port key in the example above). These keys will also be available to the command or toolTips str.format function invocation. Since we have not provided either of the verbose or workDir sections, they will automatically default to False and /tmp/cpawd-YYYYMMDD-HHMMSS . This example cpawdConfig.yaml file implements a simple multi-repository mkdocs tool, similar to monorepo . However by using cpawd to implement a multi-repository mkdocs tool, the mkdocs invocations in each repository are completely separate from each other. (Alas, when using monorepo to implement multi-repository documentation, the monorepo extension interferes with many of the other mkdocs extensions). Output When run, the cpawd command will out put a list of the configured toolTips and log files: Tool tips: webServer http://localhost:8008 --------------------------------------------------------------- Logfiles for each task: webServer tail -f /tmp/cpawd-20210724-171805/webServer/command.log moar /tmp/cpawd-20210724-171805/webServer/command.log computePods tail -f /tmp/cpawd-20210724-171805/computePods/command.log moar /tmp/cpawd-20210724-171805/computePods/command.log pythonUtils tail -f /tmp/cpawd-20210724-171805/pythonUtils/command.log moar /tmp/cpawd-20210724-171805/pythonUtils/command.log interfaces tail -f /tmp/cpawd-20210724-171805/interfaces/command.log moar /tmp/cpawd-20210724-171805/interfaces/command.log --------------------------------------------------------------- This list of toolTips and log files is then followed by a \"stream of consciousness\" list of tasks run. If you copy and paste any of the log file commands (as above) in the 'tab' of a terminal emulator, you will be able to watch the outputs of the respective tasks as they are run.","title":"ComputePods Asynchronous Watch-Do tools"},{"location":"#computepods-asynchronous-watch-do-tools","text":"This tool uses the Python asyncio , asyncinotify and aiofiles libraries to monitor a number of watch-do tasks in \"parallel\". Each watch-do task consists of a number of directories and/or files to be watched for changes. On any change, a corresponding task is run, the output captured and appended to a long running log file associated with that task. Each watch-do task should be provided with a projectDir , a list of watches (directories/files relative to the projectDir ), and a command. You can use standard Python str.format notation to format the command. The command string format will be provided with the dict of configured tasks. Each watch-do task will automatically be provided a workDir as well as a logFile (opened on the path logFilePath ), which will be located in that task's workDir . On Linux, by default a task's workDir will be located in the /tmp directory, and so will be automatically removed on each reboot.","title":"ComputePods Asynchronous Watch-Do tools"},{"location":"#installation","text":"This python tool has not (yet) been uploaded to pypi.org . So to install it you need to use: pip install git+https://github.com/computePods/asyncWatchDo/ ( see Examples 5. Install a project from VCS ) or pipx install git+https://github.com/computePods/asyncWatchDo/ ( see installing from source control ).","title":"Installation"},{"location":"#cpawd-command","text":"The cpawd command looks for its configuration in a cpawdConfig.yaml YAML file located in the directory in which the cpawd is started. The cpawd command takes three optional arguments, --verbose (to report the loaded configuration), --config (to layer on additional configuration files), and --help . dev:~/dev/computePods/asyncWatchDo$ cpawd --help usage: cpawd [-h] [-c CONFIG] [-v | --verbose | --no-verbose] [-d | --debug | --no-debug] [-p PAGER] Asynchronously watch multiple directories and perform actions on changes. optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG overlay configuration from file -v, --verbose, --no-verbose show the loaded configuration (default: False) -d, --debug, --no-debug provide debugging output (default: False) -p PAGER, --pager PAGER pager to use in task list (default=moar)","title":"cpawd command"},{"location":"#configuration-file","text":"The cpawdConfig.yaml file expects three sections: tasks : is a dict of watch-do task descriptions, each of which is itself a dict with the keys: cmd : is a list of command line arguments which will be run (using exec) whenever any change is detected. Arguments can use the python str.format to dynamically add information from the tasks dictionary. You can use this to ensure commands for one task, can use information, such as workDir , from any other configured task (see example below). projectDir : provides a single main directory from which all of the watch paths are expected to be located relatively. If no projectDir is provided, it will be assigned to the workDir . watch : is a list of paths, relative to the projectDir, to be (recursively) watched for changes. The watch paths, can, optionally, be specified with either a leading ~ or / . Watch paths with leading ~ will be relative to the user's home directory. Watch paths with leading / are assumed to be absolute paths and are not altered. Watch paths with no leading ~ or / are assumed to be relative to the projectDir. runOnce : is a key which, if it exists, ensures the command is only run once (and is never restarted on file system events). You can use this for any command which provides its own file system watch abilities. toolTips : is a string which will be echoed to the user. You can use this, for example, to inform the user of the configured URL provided by a web-server task. All other keys will be provided to the str.format when expanding either the command arguments or the toolTips (see the example below). verbose : is a Boolean which if True , will report the loaded configuration. The default is False . workDir : is a dict with the keys baseDir and prefix . This workDir is used to specify the base workDir for all of the work-do task's individual workDirs . The base workDir will be located in the baseDir and will have the name consisting of the prefix appended with the date and time the cpawd command was started. The default baseDir is /tmp and the default prefix is cpawd .","title":"Configuration file"},{"location":"#example","text":"An example cpawdConfig.yaml configuration file might be: tasks: webServer: runOnce: true toolTips: \"http://localhost:{webServer[port]}\" port: \"8008\" watch: - html cmd: - cphttp - -v - -l - debug - p - \"{webServer[port]}\" - -d - \"{webServer[workDir]}/html\" - -w - \"{webServer[workDir]}/html\" computePods: projectDir: ~/dev/computePods/computePods.github.io watch: - docs cmd: - mkdocs - --verbose - --site-dir - \"{webServer[workDir]}/html\" pythonUtils: projectDir: ~/dev/computePods/pythonUtils watch: - cputils - tests cmd: - mkdocs - --verbose - --site-dir - \"{webServer[workDir]}/html/pythonUtils\" interfaces: projectDir: ~/dev/computePods/interfaces watch: - docs - interaces cmd: - mkdocs - --verbose - --site-dir - \"{webServer[workDir]}/html/interfaces\" Notes : The {webServer[workDir]} in each of the above cmd keys will be dynamically replaced (using the str.format function) to the value of the webServer watch-do task's workDir . You can add your own keys in each of the tasks (for example the port key in the example above). These keys will also be available to the command or toolTips str.format function invocation. Since we have not provided either of the verbose or workDir sections, they will automatically default to False and /tmp/cpawd-YYYYMMDD-HHMMSS . This example cpawdConfig.yaml file implements a simple multi-repository mkdocs tool, similar to monorepo . However by using cpawd to implement a multi-repository mkdocs tool, the mkdocs invocations in each repository are completely separate from each other. (Alas, when using monorepo to implement multi-repository documentation, the monorepo extension interferes with many of the other mkdocs extensions).","title":"Example"},{"location":"#output","text":"When run, the cpawd command will out put a list of the configured toolTips and log files: Tool tips: webServer http://localhost:8008 --------------------------------------------------------------- Logfiles for each task: webServer tail -f /tmp/cpawd-20210724-171805/webServer/command.log moar /tmp/cpawd-20210724-171805/webServer/command.log computePods tail -f /tmp/cpawd-20210724-171805/computePods/command.log moar /tmp/cpawd-20210724-171805/computePods/command.log pythonUtils tail -f /tmp/cpawd-20210724-171805/pythonUtils/command.log moar /tmp/cpawd-20210724-171805/pythonUtils/command.log interfaces tail -f /tmp/cpawd-20210724-171805/interfaces/command.log moar /tmp/cpawd-20210724-171805/interfaces/command.log --------------------------------------------------------------- This list of toolTips and log files is then followed by a \"stream of consciousness\" list of tasks run. If you copy and paste any of the log file commands (as above) in the 'tab' of a terminal emulator, you will be able to watch the outputs of the respective tasks as they are run.","title":"Output"},{"location":"API/cpawd/","text":"cpawd.cli Implements the command line interface for the ComputePods Async based Watch-Do tool. cpawd () Parse the command line arguments, load the configuration, and then run the tasks using the asyncio.run method. Source code in cpawd/cpawd.py def cpawd () : \"\"\" Parse the command line arguments, load the configuration, and then run the tasks using the `asyncio.run` method. \"\"\" argparser = argparse . ArgumentParser ( description = \"Asynchronously watch multiple directories and perform actions on changes.\" ) argparser . add_argument ( \"-c\" , \"--config\" , action = 'append' , default = [], help = \"overlay configuration from file\" ) argparser . add_argument ( \"-v\" , \"--verbose\" , default = False , action = argparse . BooleanOptionalAction , help = \"show the loaded configuration\" ) argparser . add_argument ( \"-d\" , \"--debug\" , default = False , action = argparse . BooleanOptionalAction , help = \"provide debugging output\" ) argparser . add_argument ( \"-p\" , \"--pager\" , default = \"moar\" , help = \"pager to use in task list (default=moar)\" ) cliArgs = argparser . parse_args () if cliArgs . debug : logging . basicConfig ( level = logging . DEBUG ) else : logging . basicConfig ( level = logging . WARNING ) logger = logging . getLogger ( \"taskRunner\" ) config = loadConfig ( cliArgs ) loop = asyncio . get_event_loop () def signalHandler ( signum ) : \"\"\" Handle an OS system signal by stopping the debouncing tasks \"\"\" print ( \"\" ) print ( \"Shutting down...\" ) logger . info ( \"SignalHandler: Caught signal {} \" . format ( signum )) shutdownTasks . set () loop . set_debug ( cliArgs . verbose ) loop . add_signal_handler ( signal . SIGTERM , signalHandler , \"SIGTERM\" ) loop . add_signal_handler ( signal . SIGHUP , signalHandler , \"SIGHUP\" ) loop . add_signal_handler ( signal . SIGINT , signalHandler , \"SIGINT\" ) loop . run_until_complete ( runTasks ( config )) print ( \" \\n done!\" )","title":"cpawd.cli"},{"location":"API/cpawd/#cpawdcli","text":"Implements the command line interface for the ComputePods Async based Watch-Do tool.","title":"cpawd.cli"},{"location":"API/cpawd/#cpawd.cpawd.cpawd","text":"Parse the command line arguments, load the configuration, and then run the tasks using the asyncio.run method. Source code in cpawd/cpawd.py def cpawd () : \"\"\" Parse the command line arguments, load the configuration, and then run the tasks using the `asyncio.run` method. \"\"\" argparser = argparse . ArgumentParser ( description = \"Asynchronously watch multiple directories and perform actions on changes.\" ) argparser . add_argument ( \"-c\" , \"--config\" , action = 'append' , default = [], help = \"overlay configuration from file\" ) argparser . add_argument ( \"-v\" , \"--verbose\" , default = False , action = argparse . BooleanOptionalAction , help = \"show the loaded configuration\" ) argparser . add_argument ( \"-d\" , \"--debug\" , default = False , action = argparse . BooleanOptionalAction , help = \"provide debugging output\" ) argparser . add_argument ( \"-p\" , \"--pager\" , default = \"moar\" , help = \"pager to use in task list (default=moar)\" ) cliArgs = argparser . parse_args () if cliArgs . debug : logging . basicConfig ( level = logging . DEBUG ) else : logging . basicConfig ( level = logging . WARNING ) logger = logging . getLogger ( \"taskRunner\" ) config = loadConfig ( cliArgs ) loop = asyncio . get_event_loop () def signalHandler ( signum ) : \"\"\" Handle an OS system signal by stopping the debouncing tasks \"\"\" print ( \"\" ) print ( \"Shutting down...\" ) logger . info ( \"SignalHandler: Caught signal {} \" . format ( signum )) shutdownTasks . set () loop . set_debug ( cliArgs . verbose ) loop . add_signal_handler ( signal . SIGTERM , signalHandler , \"SIGTERM\" ) loop . add_signal_handler ( signal . SIGHUP , signalHandler , \"SIGHUP\" ) loop . add_signal_handler ( signal . SIGINT , signalHandler , \"SIGINT\" ) loop . run_until_complete ( runTasks ( config )) print ( \" \\n done!\" )","title":"cpawd()"},{"location":"API/fsWatcher/","text":"cpawd.fsWatcher The fsWatcher module adapts the asyncinotify example to recursively watch directories or files either by a direct request, or as they are created inside watched directories. FSWatcher The FSWatcher class manages the Linux file system inotify watches for a given collection of directories or files. It provides a file change event stream via the iterable recursive_watch method. To allow for asynchronous operation, the \"watches\" are added to an asyncio.Queue managed by the managePathsToWatchQueue method. When used, this managePathsToWatchQueue method should be run inside its own asyncio.Task . get_directories_recursive ( self , path ) Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. Source code in cpawd/fsWatcher.py def get_directories_recursive ( self , path ) : \"\"\" Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. \"\"\" if path . is_dir () : yield path for child in path . iterdir (): yield from self . get_directories_recursive ( child ) elif path . is_file () : yield path managePathsToWatchQueue ( self ) async Implement all (pending) requests to watch/unWatch a directory or file which are in the pathsToWatchQueue . When watching, the paths contained in all directories are themselves recursively added to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def managePathsToWatchQueue ( self ) : \"\"\" Implement all (pending) requests to watch/unWatch a directory or file which are in the `pathsToWatchQueue`. When watching, the paths contained in all directories are themselves recursively added to the `pathsToWatchQueue`. \"\"\" while self . continueWatchingFS : addPath , aPathToWatch , theWatch = await self . pathsToWatchQueue . get () if addPath : for aPath in self . get_directories_recursive ( Path ( aPathToWatch )) : try : self . numWatches = self . numWatches + 1 self . inotify . add_watch ( aPath , self . wrMask ) self . logger . debug ( f 'INIT: watching { aPath } ' ) except PermissionError as err : pass except Exception as err : print ( f \"Exception while trying to watch: [ { aPath } ]\" ) traceback . print_exc ( err ) # we can't watch this path just yet... # ... schedule its parent and try again... await self . watchAPath ( aPath . parent ) else : # according to the documentation.... the corresponding # Mask.IGNORE event will automatically remove this watch. #self.inotify.rm_watch(theWatch) self . numUnWatches = self . numUnWatches + 1 self . logger . debug ( f 'INIT: unWatching { aPathToWatch } ' ) if aPathToWatch in self . rootPaths : self . logger . debug ( f 'INIT: found root path... rewatching it { aPathToWatch } ' ) await self . watchAPath ( aPathToWatch ) self . pathsToWatchQueue . task_done () stopWatchingFileSystem ( self ) (Gracefully) stop watching the file system Source code in cpawd/fsWatcher.py def stopWatchingFileSystem ( self ) : \"\"\"(Gracefully) stop watching the file system\"\"\" self . continueWatchingFS = False unWatchAPath ( self , pathToWatch , aWatch ) async Add a single directory or file to be unWatched by this instance of FSWatcher to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def unWatchAPath ( self , pathToWatch , aWatch ) : \"\"\" Add a single directory or file to be unWatched by this instance of `FSWatcher` to the `pathsToWatchQueue`. \"\"\" self . logger . debug ( \"Adding path to (un)watch queue {} \" . format ( pathToWatch )) await self . pathsToWatchQueue . put (( False , pathToWatch , aWatch )) watchAPath ( self , pathToWatch ) async Add a single directory or file to be watched by this instance of FSWatcher to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def watchAPath ( self , pathToWatch ) : \"\"\" Add a single directory or file to be watched by this instance of `FSWatcher` to the `pathsToWatchQueue`. \"\"\" self . logger . debug ( \"Adding path to watch queue {} \" . format ( pathToWatch )) await self . pathsToWatchQueue . put (( True , pathToWatch , None )) watchARootPath ( self , pathToWatch ) async Add a single directory or file to the list of \"root\" paths to watch as well as schedule it to be watched. When one of the root paths is deleted, it will be re-watched. Source code in cpawd/fsWatcher.py async def watchARootPath ( self , pathToWatch ) : \"\"\"Add a single directory or file to the list of \"root\" paths to watch as well as schedule it to be watched. When one of the root paths is deleted, it will be re-watched.\"\"\" self . logger . debug ( \"Adding root path [ {} ]\" . format ( pathToWatch )) self . rootPaths . append ( pathToWatch ) await self . watchAPath ( pathToWatch ) watchForFileSystemEvents ( self ) An asynchronously interable method which yields file system change events. Source code in cpawd/fsWatcher.py async def watchForFileSystemEvents ( self ): \"\"\" An asynchronously interable method which yields file system change events. \"\"\" # Things that can throw this off: # # * Moving a watched directory out of the watch tree (will still # generate events even when outside of directory tree) # # * Doing two changes on a directory or something before the program # has a time to handle it (this will also throw off a lot of inotify # code, though) # # * Moving a watched directory within a watched directory will get the # wrong path. This needs to use the cookie system to link events # together and complete the move properly, which can still make some # events get the wrong path if you get file events during the move or # something silly like that, since MOVED_FROM and MOVED_TO aren't # guaranteed to be contiguous. That exercise is left up to the # reader. # # * Trying to watch a path that doesn't exist won't automatically # create it or anything of the sort. # # * Deleting and recreating or moving the watched directory won't do # anything special, but it probably should. # async for event in self . inotify : if not self . continueWatchingFS : return # If this is a creation event, add a watch for the new path (and its # subdirectories if any) # if Mask . CREATE in event . mask and event . path is not None : await self . watchAPath ( event . path ) if Mask . DELETE_SELF in event . mask and event . path is not None : await self . unWatchAPath ( event . path , event . watch ) # If there are some bits in the cpMask in the event.mask yield this # event # if event . mask & self . cpMask : yield event else : # Note that these events are needed for cleanup purposes. # We'll always get IGNORED events so the watch can be removed # from the inotify. We don't need to do anything with the # events, but they do need to be generated for cleanup. # We don't need to pass IGNORED events up, because the end-user # doesn't have the inotify instance anyway, and IGNORED is just # used for management purposes. # self . logger . debug ( f 'UNYIELDED EVENT: { event } ' ) getMaskName ( aMask ) Translate a raw Mask number into a human readable name. Source code in cpawd/fsWatcher.py def getMaskName ( aMask ) : \"\"\" Translate a raw Mask number into a human readable name. \"\"\" maskName = \"\" for aPotentialMask in masksToListenFor : if aMask & aPotentialMask [ 0 ] : maskName = maskName + ' ' + aPotentialMask [ 1 ] return maskName . lstrip () getMasksToListenFor () Compute the module's list of masksToLisentFor into a single mask. Source code in cpawd/fsWatcher.py def getMasksToListenFor () : \"\"\" Compute the module's list of `masksToLisentFor` into a single mask. \"\"\" masks = 0 for aMask in masksToListenFor : masks = masks | aMask [ 0 ] return masks","title":"cpawd.fsWatcher"},{"location":"API/fsWatcher/#cpawdfswatcher","text":"The fsWatcher module adapts the asyncinotify example to recursively watch directories or files either by a direct request, or as they are created inside watched directories.","title":"cpawd.fsWatcher"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher","text":"The FSWatcher class manages the Linux file system inotify watches for a given collection of directories or files. It provides a file change event stream via the iterable recursive_watch method. To allow for asynchronous operation, the \"watches\" are added to an asyncio.Queue managed by the managePathsToWatchQueue method. When used, this managePathsToWatchQueue method should be run inside its own asyncio.Task .","title":"FSWatcher"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.get_directories_recursive","text":"Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. Source code in cpawd/fsWatcher.py def get_directories_recursive ( self , path ) : \"\"\" Recursively list all directories under path, including path itself, if it's a directory. The path itself is always yielded before its children are iterated, so you can pre-process a path (by watching it with inotify) before you get the directory listing. \"\"\" if path . is_dir () : yield path for child in path . iterdir (): yield from self . get_directories_recursive ( child ) elif path . is_file () : yield path","title":"get_directories_recursive()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.managePathsToWatchQueue","text":"Implement all (pending) requests to watch/unWatch a directory or file which are in the pathsToWatchQueue . When watching, the paths contained in all directories are themselves recursively added to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def managePathsToWatchQueue ( self ) : \"\"\" Implement all (pending) requests to watch/unWatch a directory or file which are in the `pathsToWatchQueue`. When watching, the paths contained in all directories are themselves recursively added to the `pathsToWatchQueue`. \"\"\" while self . continueWatchingFS : addPath , aPathToWatch , theWatch = await self . pathsToWatchQueue . get () if addPath : for aPath in self . get_directories_recursive ( Path ( aPathToWatch )) : try : self . numWatches = self . numWatches + 1 self . inotify . add_watch ( aPath , self . wrMask ) self . logger . debug ( f 'INIT: watching { aPath } ' ) except PermissionError as err : pass except Exception as err : print ( f \"Exception while trying to watch: [ { aPath } ]\" ) traceback . print_exc ( err ) # we can't watch this path just yet... # ... schedule its parent and try again... await self . watchAPath ( aPath . parent ) else : # according to the documentation.... the corresponding # Mask.IGNORE event will automatically remove this watch. #self.inotify.rm_watch(theWatch) self . numUnWatches = self . numUnWatches + 1 self . logger . debug ( f 'INIT: unWatching { aPathToWatch } ' ) if aPathToWatch in self . rootPaths : self . logger . debug ( f 'INIT: found root path... rewatching it { aPathToWatch } ' ) await self . watchAPath ( aPathToWatch ) self . pathsToWatchQueue . task_done ()","title":"managePathsToWatchQueue()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.stopWatchingFileSystem","text":"(Gracefully) stop watching the file system Source code in cpawd/fsWatcher.py def stopWatchingFileSystem ( self ) : \"\"\"(Gracefully) stop watching the file system\"\"\" self . continueWatchingFS = False","title":"stopWatchingFileSystem()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.unWatchAPath","text":"Add a single directory or file to be unWatched by this instance of FSWatcher to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def unWatchAPath ( self , pathToWatch , aWatch ) : \"\"\" Add a single directory or file to be unWatched by this instance of `FSWatcher` to the `pathsToWatchQueue`. \"\"\" self . logger . debug ( \"Adding path to (un)watch queue {} \" . format ( pathToWatch )) await self . pathsToWatchQueue . put (( False , pathToWatch , aWatch ))","title":"unWatchAPath()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.watchAPath","text":"Add a single directory or file to be watched by this instance of FSWatcher to the pathsToWatchQueue . Source code in cpawd/fsWatcher.py async def watchAPath ( self , pathToWatch ) : \"\"\" Add a single directory or file to be watched by this instance of `FSWatcher` to the `pathsToWatchQueue`. \"\"\" self . logger . debug ( \"Adding path to watch queue {} \" . format ( pathToWatch )) await self . pathsToWatchQueue . put (( True , pathToWatch , None ))","title":"watchAPath()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.watchARootPath","text":"Add a single directory or file to the list of \"root\" paths to watch as well as schedule it to be watched. When one of the root paths is deleted, it will be re-watched. Source code in cpawd/fsWatcher.py async def watchARootPath ( self , pathToWatch ) : \"\"\"Add a single directory or file to the list of \"root\" paths to watch as well as schedule it to be watched. When one of the root paths is deleted, it will be re-watched.\"\"\" self . logger . debug ( \"Adding root path [ {} ]\" . format ( pathToWatch )) self . rootPaths . append ( pathToWatch ) await self . watchAPath ( pathToWatch )","title":"watchARootPath()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.FSWatcher.watchForFileSystemEvents","text":"An asynchronously interable method which yields file system change events. Source code in cpawd/fsWatcher.py async def watchForFileSystemEvents ( self ): \"\"\" An asynchronously interable method which yields file system change events. \"\"\" # Things that can throw this off: # # * Moving a watched directory out of the watch tree (will still # generate events even when outside of directory tree) # # * Doing two changes on a directory or something before the program # has a time to handle it (this will also throw off a lot of inotify # code, though) # # * Moving a watched directory within a watched directory will get the # wrong path. This needs to use the cookie system to link events # together and complete the move properly, which can still make some # events get the wrong path if you get file events during the move or # something silly like that, since MOVED_FROM and MOVED_TO aren't # guaranteed to be contiguous. That exercise is left up to the # reader. # # * Trying to watch a path that doesn't exist won't automatically # create it or anything of the sort. # # * Deleting and recreating or moving the watched directory won't do # anything special, but it probably should. # async for event in self . inotify : if not self . continueWatchingFS : return # If this is a creation event, add a watch for the new path (and its # subdirectories if any) # if Mask . CREATE in event . mask and event . path is not None : await self . watchAPath ( event . path ) if Mask . DELETE_SELF in event . mask and event . path is not None : await self . unWatchAPath ( event . path , event . watch ) # If there are some bits in the cpMask in the event.mask yield this # event # if event . mask & self . cpMask : yield event else : # Note that these events are needed for cleanup purposes. # We'll always get IGNORED events so the watch can be removed # from the inotify. We don't need to do anything with the # events, but they do need to be generated for cleanup. # We don't need to pass IGNORED events up, because the end-user # doesn't have the inotify instance anyway, and IGNORED is just # used for management purposes. # self . logger . debug ( f 'UNYIELDED EVENT: { event } ' )","title":"watchForFileSystemEvents()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.getMaskName","text":"Translate a raw Mask number into a human readable name. Source code in cpawd/fsWatcher.py def getMaskName ( aMask ) : \"\"\" Translate a raw Mask number into a human readable name. \"\"\" maskName = \"\" for aPotentialMask in masksToListenFor : if aMask & aPotentialMask [ 0 ] : maskName = maskName + ' ' + aPotentialMask [ 1 ] return maskName . lstrip ()","title":"getMaskName()"},{"location":"API/fsWatcher/#cpawd.fsWatcher.getMasksToListenFor","text":"Compute the module's list of masksToLisentFor into a single mask. Source code in cpawd/fsWatcher.py def getMasksToListenFor () : \"\"\" Compute the module's list of `masksToLisentFor` into a single mask. \"\"\" masks = 0 for aMask in masksToListenFor : masks = masks | aMask [ 0 ] return masks","title":"getMasksToListenFor()"},{"location":"API/loadConfiguration/","text":"cpawd.loadConfiguration Load and normalise the cpawd configuration loadConfig ( cliArgs ) Load the configuration by merging any cpawdConfig.yaml found in the current working directory, and then any other configuration files specified on the command line. Then perform the following normalisation: The base working directory is computed using the baseDir and prefix keys found in the workDir section of the merged configuration. Compute workDir for each task in the tasks section of the merged configuration. Ensure all workDir exists (both for the base and the individual tasks) Expand all watched paths to an absolute path in the file system. Check that the projectDir exists for each task. Compute logFilePaths and open logFiles for each task. Source code in cpawd/loadConfiguration.py def loadConfig ( cliArgs ) : \"\"\" Load the configuration by merging any `cpawdConfig.yaml` found in the current working directory, and then any other configuration files specified on the command line. Then perform the following normalisation: - The base working directory is computed using the `baseDir` and `prefix` keys found in the `workDir` section of the merged configuration. - Compute `workDir` for each task in the `tasks` section of the merged configuration. - Ensure all `workDir` exists (both for the base and the individual tasks) - Expand all watched paths to an absolute path in the file system. - Check that the `projectDir` exists for each task. - Compute logFilePaths and open logFiles for each task. \"\"\" config = { 'workDir' : { 'baseDir' : '/tmp' , 'prefix' : 'cpawd' }, 'tasks' : {}, 'verbose' : False } if cliArgs . verbose : config [ 'verbose' ] = cliArgs . verbose if cliArgs . debug : config [ 'debug' ] = cliArgs . debug cliArgs . config . insert ( 0 , 'cpawdConfig.yaml' ) for aConfigPath in cliArgs . config : if os . path . exists ( aConfigPath ) : try : with open ( aConfigPath ) as aConfigFile : aConfig = yaml . safe_load ( aConfigFile . read ()) mergeYamlData ( config , aConfig , \"\" ) except Exception as err : print ( \"Could not load configuration from [ {} ]\" . format ( aConfigPath )) print ( err ) # create the working directory if 'workDir' not in config [ 'workDir' ] : config [ 'workDir' ][ 'workDir' ] = os . path . join ( config [ 'workDir' ][ 'baseDir' ], config [ 'workDir' ][ 'prefix' ] + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ) ) workDir = config [ 'workDir' ][ 'workDir' ] if os . path . exists ( workDir ) : shutil . rmtree ( workDir ) os . makedirs ( workDir ) # ensure the task work and project directories exist for aTaskName , aTask in config [ 'tasks' ] . items () : aTask [ 'workDir' ] = os . path . join ( workDir , aTaskName ) os . makedirs ( aTask [ 'workDir' ]) aTask [ 'logFilePath' ] = os . path . join ( workDir , aTaskName , 'command.log' ) if 'projectDir' in aTask : aTask [ 'projectDir' ] = os . path . abspath ( os . path . expanduser ( aTask [ 'projectDir' ])) else : aTask [ 'projectDir' ] = aTask [ 'workDir' ] if not os . path . exists ( aTask [ 'projectDir' ]) : taskError ( \"the projectDir for task {} MUST exist in the file system\" . format ( aTaskName ), aTask ) if 'watch' not in aTask or not aTask [ 'watch' ] : taskError ( \"all tasks MUST have a collection of files/directories to watch \\n no 'watch' list provided in task [ {} ]:\" . format ( aTaskName ), aTask ) expandedWatches = [] for aWatch in aTask [ 'watch' ] : newWatch = os . path . expanduser ( aWatch ) if not newWatch . startswith ( '/' ) : newWatch = os . path . join ( aTask [ 'projectDir' ], newWatch ) os . makedirs ( newWatch , exist_ok = True ) expandedWatches . append ( newWatch ) aTask [ 'watch' ] = expandedWatches # expand toolTips and commands for aTaskName , aTask in config [ 'tasks' ] . items () : if 'cmd' not in aTask : taskError ( \"all tasks MUST have a cmd; no cmd provied in task [ {} ]\" . format ( aTaskName ), aTask ) if type ( aTask [ 'cmd' ]) is not list : taskError ( \"task cmds MUST be a list of command followed by arguments \\n found type: {} in task {} \" . format ( type ( aTask [ 'cmd' ]), aTaskName ), aTask ) try : newCmd = [] for anArgument in aTask [ 'cmd' ] : newCmd . append ( anArgument . format ( ** config [ 'tasks' ])) aTask [ 'cmd' ] = newCmd except Exception as err : print ( \"Could not expand variables in cmd string:\" ) print ( yaml . dump ( aTask [ 'cmd' ])) print ( repr ( err )) if 'toolTips' in aTask : try : aTask [ 'toolTips' ] = aTask [ 'toolTips' ] . format ( ** config [ 'tasks' ]) except Exception as err : print ( \"Could not expand variables in toolTips string:\" ) print ( yaml . dump ( aTask [ 'toolTips' ])) print ( repr ( err )) if config [ 'verbose' ] : print ( \"configuration:\" ) print ( \"---------------------------------------------------------------\" ) print ( yaml . dump ( config )) # announce User Messages print ( \"---------------------------------------------------------------\" ) print ( \" \\n Tool tips: \\n \" ) for aTaskName , aTask in config [ 'tasks' ] . items () : if 'toolTips' in aTask : print ( \" {} \\n {} \" . format ( aTaskName , aTask [ 'toolTips' ])) # announce log files print ( \" \\n ---------------------------------------------------------------\" ) print ( \" \\n Logfiles for each task: \\n \" ) for aTaskName , aTask in config [ 'tasks' ] . items () : print ( \" {} \\n tail -f {} \" . format ( aTaskName , aTask [ 'logFilePath' ])) print ( \" {} {} \" . format ( cliArgs . pager , aTask [ 'logFilePath' ])) print ( \"\" ) print ( \"---------------------------------------------------------------\" ) print ( \"\" ) return config mergeYamlData ( yamlData , newYamlData , thePath ) This is a generic Python merge. It is a deep merge and handles both dictionaries and arrays Source code in cpawd/loadConfiguration.py def mergeYamlData ( yamlData , newYamlData , thePath ) : \"\"\" This is a generic Python merge. It is a *deep* merge and handles both dictionaries and arrays \"\"\" if type ( yamlData ) is None : print ( \"ERROR yamlData should NEVER be None \" ) sys . exit ( - 1 ) if type ( yamlData ) != type ( newYamlData ) : print ( \"Incompatible types {} and {} while trying to merge YAML data at {} \" . format ( type ( yamlData ), type ( newYamlData ), thePath )) print ( \"Stoping merge at {} \" . format ( thePath )) return if type ( yamlData ) is dict : for key , value in newYamlData . items () : if key not in yamlData : yamlData [ key ] = value elif type ( yamlData [ key ]) is dict : mergeYamlData ( yamlData [ key ], value , thePath + '.' + key ) elif type ( yamlData [ key ]) is list : for aValue in value : yamlData [ key ] . append ( aValue ) else : yamlData [ key ] = value elif type ( yamlData ) is list : for value in newYamlData : yamlData . append ( value ) else : print ( \"ERROR yamlData MUST be either a dictionary or an array.\" ) sys . exit ( - 1 )","title":"cpawd.loadConfiguration"},{"location":"API/loadConfiguration/#cpawdloadconfiguration","text":"Load and normalise the cpawd configuration","title":"cpawd.loadConfiguration"},{"location":"API/loadConfiguration/#cpawd.loadConfiguration.loadConfig","text":"Load the configuration by merging any cpawdConfig.yaml found in the current working directory, and then any other configuration files specified on the command line. Then perform the following normalisation: The base working directory is computed using the baseDir and prefix keys found in the workDir section of the merged configuration. Compute workDir for each task in the tasks section of the merged configuration. Ensure all workDir exists (both for the base and the individual tasks) Expand all watched paths to an absolute path in the file system. Check that the projectDir exists for each task. Compute logFilePaths and open logFiles for each task. Source code in cpawd/loadConfiguration.py def loadConfig ( cliArgs ) : \"\"\" Load the configuration by merging any `cpawdConfig.yaml` found in the current working directory, and then any other configuration files specified on the command line. Then perform the following normalisation: - The base working directory is computed using the `baseDir` and `prefix` keys found in the `workDir` section of the merged configuration. - Compute `workDir` for each task in the `tasks` section of the merged configuration. - Ensure all `workDir` exists (both for the base and the individual tasks) - Expand all watched paths to an absolute path in the file system. - Check that the `projectDir` exists for each task. - Compute logFilePaths and open logFiles for each task. \"\"\" config = { 'workDir' : { 'baseDir' : '/tmp' , 'prefix' : 'cpawd' }, 'tasks' : {}, 'verbose' : False } if cliArgs . verbose : config [ 'verbose' ] = cliArgs . verbose if cliArgs . debug : config [ 'debug' ] = cliArgs . debug cliArgs . config . insert ( 0 , 'cpawdConfig.yaml' ) for aConfigPath in cliArgs . config : if os . path . exists ( aConfigPath ) : try : with open ( aConfigPath ) as aConfigFile : aConfig = yaml . safe_load ( aConfigFile . read ()) mergeYamlData ( config , aConfig , \"\" ) except Exception as err : print ( \"Could not load configuration from [ {} ]\" . format ( aConfigPath )) print ( err ) # create the working directory if 'workDir' not in config [ 'workDir' ] : config [ 'workDir' ][ 'workDir' ] = os . path . join ( config [ 'workDir' ][ 'baseDir' ], config [ 'workDir' ][ 'prefix' ] + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ) ) workDir = config [ 'workDir' ][ 'workDir' ] if os . path . exists ( workDir ) : shutil . rmtree ( workDir ) os . makedirs ( workDir ) # ensure the task work and project directories exist for aTaskName , aTask in config [ 'tasks' ] . items () : aTask [ 'workDir' ] = os . path . join ( workDir , aTaskName ) os . makedirs ( aTask [ 'workDir' ]) aTask [ 'logFilePath' ] = os . path . join ( workDir , aTaskName , 'command.log' ) if 'projectDir' in aTask : aTask [ 'projectDir' ] = os . path . abspath ( os . path . expanduser ( aTask [ 'projectDir' ])) else : aTask [ 'projectDir' ] = aTask [ 'workDir' ] if not os . path . exists ( aTask [ 'projectDir' ]) : taskError ( \"the projectDir for task {} MUST exist in the file system\" . format ( aTaskName ), aTask ) if 'watch' not in aTask or not aTask [ 'watch' ] : taskError ( \"all tasks MUST have a collection of files/directories to watch \\n no 'watch' list provided in task [ {} ]:\" . format ( aTaskName ), aTask ) expandedWatches = [] for aWatch in aTask [ 'watch' ] : newWatch = os . path . expanduser ( aWatch ) if not newWatch . startswith ( '/' ) : newWatch = os . path . join ( aTask [ 'projectDir' ], newWatch ) os . makedirs ( newWatch , exist_ok = True ) expandedWatches . append ( newWatch ) aTask [ 'watch' ] = expandedWatches # expand toolTips and commands for aTaskName , aTask in config [ 'tasks' ] . items () : if 'cmd' not in aTask : taskError ( \"all tasks MUST have a cmd; no cmd provied in task [ {} ]\" . format ( aTaskName ), aTask ) if type ( aTask [ 'cmd' ]) is not list : taskError ( \"task cmds MUST be a list of command followed by arguments \\n found type: {} in task {} \" . format ( type ( aTask [ 'cmd' ]), aTaskName ), aTask ) try : newCmd = [] for anArgument in aTask [ 'cmd' ] : newCmd . append ( anArgument . format ( ** config [ 'tasks' ])) aTask [ 'cmd' ] = newCmd except Exception as err : print ( \"Could not expand variables in cmd string:\" ) print ( yaml . dump ( aTask [ 'cmd' ])) print ( repr ( err )) if 'toolTips' in aTask : try : aTask [ 'toolTips' ] = aTask [ 'toolTips' ] . format ( ** config [ 'tasks' ]) except Exception as err : print ( \"Could not expand variables in toolTips string:\" ) print ( yaml . dump ( aTask [ 'toolTips' ])) print ( repr ( err )) if config [ 'verbose' ] : print ( \"configuration:\" ) print ( \"---------------------------------------------------------------\" ) print ( yaml . dump ( config )) # announce User Messages print ( \"---------------------------------------------------------------\" ) print ( \" \\n Tool tips: \\n \" ) for aTaskName , aTask in config [ 'tasks' ] . items () : if 'toolTips' in aTask : print ( \" {} \\n {} \" . format ( aTaskName , aTask [ 'toolTips' ])) # announce log files print ( \" \\n ---------------------------------------------------------------\" ) print ( \" \\n Logfiles for each task: \\n \" ) for aTaskName , aTask in config [ 'tasks' ] . items () : print ( \" {} \\n tail -f {} \" . format ( aTaskName , aTask [ 'logFilePath' ])) print ( \" {} {} \" . format ( cliArgs . pager , aTask [ 'logFilePath' ])) print ( \"\" ) print ( \"---------------------------------------------------------------\" ) print ( \"\" ) return config","title":"loadConfig()"},{"location":"API/loadConfiguration/#cpawd.loadConfiguration.mergeYamlData","text":"This is a generic Python merge. It is a deep merge and handles both dictionaries and arrays Source code in cpawd/loadConfiguration.py def mergeYamlData ( yamlData , newYamlData , thePath ) : \"\"\" This is a generic Python merge. It is a *deep* merge and handles both dictionaries and arrays \"\"\" if type ( yamlData ) is None : print ( \"ERROR yamlData should NEVER be None \" ) sys . exit ( - 1 ) if type ( yamlData ) != type ( newYamlData ) : print ( \"Incompatible types {} and {} while trying to merge YAML data at {} \" . format ( type ( yamlData ), type ( newYamlData ), thePath )) print ( \"Stoping merge at {} \" . format ( thePath )) return if type ( yamlData ) is dict : for key , value in newYamlData . items () : if key not in yamlData : yamlData [ key ] = value elif type ( yamlData [ key ]) is dict : mergeYamlData ( yamlData [ key ], value , thePath + '.' + key ) elif type ( yamlData [ key ]) is list : for aValue in value : yamlData [ key ] . append ( aValue ) else : yamlData [ key ] = value elif type ( yamlData ) is list : for value in newYamlData : yamlData . append ( value ) else : print ( \"ERROR yamlData MUST be either a dictionary or an array.\" ) sys . exit ( - 1 )","title":"mergeYamlData()"},{"location":"API/taskRunner/","text":"cpawd.taskRunner This cpawd.taskRunner module implements the running of all watch-do tasks. The following description is illustrated in the interaction diagram below. The top level runTasks method initiates an asyncio.Tasks running the watchDo method for each watch-do task. The watchDo method reStart s an asyncio.Task , taskRunner , via a call to asyncio.ensure_future , to manage a (potentially long running) OS process. The watchDo task listens, in an FSWatcher.watchForFileSystemEvents loop, for any file system changes which might be happening, reStart ing the taskRunner task on any such changes. The taskRunner task starts by sleeping, during which time the taskRunner task can be cancelled and reStart ed (by the watchDo task). This short timeout period of cancel-able sleep acts as a debouncing timer. It allows the watchDo task to frequently reStart the taskRunner task without actually running the external OS process until any nearly simultaneous file system changes have stopped. If the taskRunner is not cancelled during the sleep, the taskRunner starts the OS process, using a call to asyncio.create_subprocess_exec , and then creates two further asyncio.Tasks , captureStdout and captureRetCode , to manage the process's output as well as to wait for the process's return code. Once an external OS process has been started, any reStart requests from the watchDo task, signals the captureStdout task to stop listening for the process's stdout, and then sends a SIGHUP signal to the process (which must respond by gracefully exiting). The watchDo task then wait s on the taskRunner task to finish before creating a new taskRunner task (and potentially repeating this cycle). The main cpawd task, can at any time request that the runTasks task shutdown. To shutdown, the runTasks task first signals all of the watchDo FSWatcher.watchForFileSystemEvents loops to stop watching for file system events. Then the runTasks task signals all running taskRunners to stop. In this interaction diagram, each asyncio.Task is represented by the function which the task runs. The OSproc thread is an external OS process, which is the ultimate \"task\" of a given watch-do task. sequenceDiagram participant main participant runTasks participant watchDo participant taskRunner participant captureRetCode participant captureStdout participant OSproc activate main main-->>runTasks: run activate runTasks runTasks-->>watchDo: create_task activate watchDo Note over watchDo,OSproc: running (one watchDo for each watch-do task) ... watchDo-->>watchDo: reStart watchDo-->>watchDo: stopTaskProc Note over watchDo,OSproc: no task/proc running so stopTaskProc does nothing watchDo-->>taskRunner: ensure_future activate taskRunner taskRunner-->>taskRunner: sleep Note over taskRunner: a watchDo reStart<br/>at this point<br/>can cancel<br/>taskRunner<br/>while sleeping taskRunner-->>OSproc: exec activate OSproc taskRunner-->>captureStdout: wait on created task activate captureStdout taskRunner-->>captureRetCode: wait on created task activate captureRetCode OSproc-->>captureStdout: stdout OSproc-->>captureStdout: stdout OSproc-->>captureStdout: stdout Note over watchDo: file system<br/>change detected watchDo-->>watchDo: reStart watchDo-->>watchDo: stopTaskProc watchDo-->>OSproc: send SIGHUP OSproc-->>captureRetCode: finished deactivate OSproc captureRetCode-->>taskRunner: finished deactivate captureRetCode watchDo-->>captureStdout: continueCapturingStdout = False captureStdout-->>taskRunner: finished deactivate captureStdout watchDo-->>taskRunner: wait taskRunner-->>watchDo: done deactivate taskRunner watchDo-->>taskRunner: ensure_future activate taskRunner Note over taskRunner,OSproc: new taskRunner starts ... taskRunner-->>watchDo: done deactivate taskRunner main-->>runTasks: shutdown runTasks-->>watchDo: stop listening for<br/>file system events runTasks-->>watchDo: stop debouncing timers watchDo-->>watchDo: stopTaskProc watchDo-->>runTasks: done deactivate watchDo runTasks-->>main: done deactivate runTasks deactivate main DebouncingTimer The DebouncingTimer class implements a simple timer to ensure multiple file system events result in only one invocation of the task command. __init__ ( self , timeout , taskName , taskDetails , taskLog , terminateSignal ) special Create the timer with a specific timeout and task definition. The taskDetails provides the command to run, the log file used to record command output, as well as the project directory in which to run the command. Source code in cpawd/taskRunner.py def __init__ ( self , timeout , taskName , taskDetails , taskLog , terminateSignal ) : \"\"\" Create the timer with a specific timeout and task definition. The taskDetails provides the command to run, the log file used to record command output, as well as the project directory in which to run the command. \"\"\" self . timeout = timeout self . taskName = taskName self . taskCmd = taskDetails [ 'cmd' ] self . taskCmdStr = \" \" . join ( taskDetails [ 'cmd' ]) self . taskLog = taskLog self . taskDir = taskDetails [ 'projectDir' ] self . termSignal = terminateSignal self . taskFuture = None self . proc = None self . pid = None self . retCode = None self . continueCapturingStdout = True cancelTimer ( self ) Cancel the Debouncing timer Source code in cpawd/taskRunner.py def cancelTimer ( self ) : \"\"\"Cancel the Debouncing timer\"\"\" if self . taskFuture and not self . procIsRunning () : logger . debug ( \"Cancelling timer for {} \" . format ( self . taskName )) self . taskFuture . cancel () captureOutput ( self ) async Capture the (stdout) output from the external process Source code in cpawd/taskRunner.py async def captureOutput ( self ) : \"\"\"Capture the (stdout) output from the external process\"\"\" logger . debug ( \"CaptureOutput task running for {} \" . format ( self . taskName )) taskLog = self . taskLog if self . proc is not None : stdout = self . proc . stdout if stdout : await taskLog . write ( \" \\n ============================================================================ \\n \" ) await taskLog . write ( \" {} ( {} ) stdout @ {} \\n \" . format ( self . taskName , self . proc . pid , time . strftime ( \"%Y/%m/ %d %H:%M:%S\" ) )) await taskLog . write ( \" {} \\n \" . format ( self . taskCmdStr )) await taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) await taskLog . flush () while self . continueCapturingStdout and not stdout . at_eof () : logger . debug ( \"Collecting {} stdout ( {} )\" . format ( self . taskName , self . proc . pid )) aLine = await stdout . readline () await taskLog . write ( aLine . decode ()) await taskLog . flush () if self . continueCapturingStdout : logger . debug ( \"Finshed collecting {} stdout ( {} )\" . format ( self . taskName , self . proc . pid )) else : await taskLog . write ( \" \\n [Stopped collecting stdout]\" ) logger . debug ( \"Stopped collecting process stdout for {} ( {} )\" . format ( self . taskName , self . pid )) await taskLog . write ( \" \\n ---------------------------------------------------------------------------- \\n \" ) await taskLog . write ( \" {} ( {} ) stdout @ {} \\n \" . format ( self . taskName , self . pid , time . strftime ( \"%Y/%m/ %d %H:%M:%S\" ) )) await taskLog . flush () else : logger . debug ( \"No stdout found for {} \" . format ( self . taskName )) else : logger . debug ( \"No external process found so no stdout captured for {} \" . format ( self . taskName )) logger . debug ( \"CaptureOutput task finished for {} \" . format ( self . taskName )) captureRetCode ( self ) async Wait for and capture the return code of the external process Source code in cpawd/taskRunner.py async def captureRetCode ( self ) : \"\"\"Wait for and capture the return code of the external process\"\"\" logger . debug ( \"Capturing return code for {} \" . format ( self . taskName )) try : self . retCode = await self . proc . wait () except ProcessLookupError : logger . debug ( \"No process found for {} (pid: {} )\" . format ( self . taskName , self . pid )) if self . retCode is not None : retCode = self . retCode pid = self . pid logger . debug ( \"Return code for {} is {} (pid: {} )\" . format ( self . taskName , retCode , pid )) taskLog = self . taskLog await taskLog . write ( \" {} task ( {} ) exited with {} \\n \" . format ( self . taskName , pid , retCode )) await taskLog . write ( \" \\n \" ) await taskLog . flush () logger . debug ( \"Finished {} ( {} ) command [ {} ] exited with {} \" . format ( self . taskName , pid , self . taskCmdStr , retCode )) self . proc = None logger . debug ( \"Captured return code for {} \" . format ( self . taskName )) procIsRunning ( self ) Determine if an external process is (still) running Source code in cpawd/taskRunner.py def procIsRunning ( self ) : \"\"\"Determine if an external process is (still) running\"\"\" return self . proc is not None and self . proc . returncode is None reStart ( self ) async (Re)Start the timer. If the timer is already started, it is restarted with a new timeout period. Source code in cpawd/taskRunner.py async def reStart ( self ) : \"\"\" (Re)Start the timer. If the timer is already started, it is restarted with a new timeout period. \"\"\" await self . stopTaskProc () if self . taskFuture : self . cancelTimer () if not self . taskFuture . done () : logger . debug ( \"Waiting for the previous taskRunner task for {} to finish\" . format ( self . taskName )) await asyncio . wait ([ self . taskFuture ]) logger . debug ( \"Starting new taskRunner for {} \" . format ( self . taskName )) self . taskFuture = asyncio . ensure_future ( self . taskRunner ()) stopTaskProc ( self ) async Stop the external process Source code in cpawd/taskRunner.py async def stopTaskProc ( self ) : \"\"\"Stop the external process\"\"\" logger . debug ( \"Attempting to stop the task process for {} \" . format ( self . taskName )) self . continueCapturingStdout = False if self . proc is not None : pid = self . proc . pid logger . debug ( \"Process found for {} ( {} )\" . format ( self . taskName , pid )) if self . procIsRunning () : logger . debug ( \"Process still running for {} \" . format ( self . taskName )) try : logger . debug ( \"Sending OS signal ( {} ) to {} (pid: {} )\" . format ( self . termSignal , self . taskName , pid )) self . proc . send_signal ( self . termSignal ) except ProcessLookupError : logger . debug ( \"No exiting external process found for {} (pid: {} )\" . format ( self . taskName , pid )) except Exception as err : logger . error ( \"Could not send signal ( {} ) to proc for {} (})\" . format ( self . termSignal , self . taskName , pid )) logger . error ( repr ( err )) traceback . print_exc () else : self . retCode = self . proc . returncode logger . debug ( \"Process finished with return code {} for {} \" . format ( self . retCode , self . taskName )) else : logger . debug ( \"No external process found for {} \" . format ( self . taskName )) taskRunner ( self ) async Run the task's command, after sleeping for the timeout period, using asyncio.create_subprocess_exec command. Source code in cpawd/taskRunner.py async def taskRunner ( self ) : \"\"\" Run the task's command, after sleeping for the timeout period, using `asyncio.create_subprocess_exec` command. \"\"\" try : logger . debug ( \"TaskRunner for {} sleeping for {} \" . format ( self . taskName , self . timeout )) await asyncio . sleep ( self . timeout ) # Now we can run the new task... # logger . debug ( \"Running {} command [ {} ]\" . format ( self . taskName , self . taskCmdStr )) self . proc = await asyncio . create_subprocess_exec ( * self . taskCmd , stdout = asyncio . subprocess . PIPE , stderr = asyncio . subprocess . STDOUT , cwd = self . taskDir ) self . pid = self . proc . pid self . retCode = None self . continueCapturingStdout = True print ( f 'Ran: { self . taskName } ' ) #await asyncio.gather( await self . captureOutput (), await self . captureRetCode () #) if self . retCode is None or self . retCode != 0 : print ( f \"FAILED: { self . taskName } ( { self . retCode } )\" ) except Exception as err : print ( \"Caught exception while running {} task\" . format ( self . taskName )) print ( repr ( err )) traceback . print_exc () runTasks ( config ) async Walk through the list of watch-do tasks and create an asyncio.Task for each task, using an invocation of the watchDo method to wrap each task. Since these tasks are not Python-CPU bound, they will essentially \"run\" in parallel. Source code in cpawd/taskRunner.py async def runTasks ( config ) : \"\"\" Walk through the list of watch-do tasks and create an `asyncio.Task` for each task, using an invocation of the `watchDo` method to wrap each task. Since these tasks are not Python-CPU bound, they will essentially \"run\" in parallel. \"\"\" for aTaskName , aTask in config [ 'tasks' ] . items () : asyncio . create_task ( watchDo ( aTaskName , aTask )) await waitForShutdown () stopTasks () async Stop all watch-do tasks Source code in cpawd/taskRunner.py async def stopTasks () : \"\"\"Stop all watch-do tasks\"\"\" logger . info ( \"Stopping all tasks\" ) for aWatcher in watchers : aWatcher . stopWatchingFileSystem () for aTimer in debouncingTimers : await aTimer . stopTaskProc () aTimer . cancelTimer () logger . debug ( \"All tasks Stoped\" ) waitForShutdown () async Wait for the shutdown event and then stop all watch-do tasks Source code in cpawd/taskRunner.py async def waitForShutdown () : \"\"\"Wait for the shutdown event and then stop all watch-do tasks\"\"\" logger . debug ( \"waiting for eventual shutdown event\" ) await shutdownTasks . wait () logger . debug ( \"got shutdown\" ) await stopTasks () logger . debug ( \"shutdown\" ) watchDo ( aTaskName , aTask ) async Setup and manage the watches, and then run the task's command using the DebouncingTimer whenever a change is detected in a watched directory or file. Source code in cpawd/taskRunner.py async def watchDo ( aTaskName , aTask ) : \"\"\" Setup and manage the watches, and then run the task's command using the DebouncingTimer whenever a change is detected in a watched directory or file. \"\"\" logger . debug ( \"Starting watchDo for {} \" . format ( aTaskName )) aWatcher = FSWatcher ( logger ) watchers . append ( aWatcher ) taskLog = await aiofiles . open ( aTask [ 'logFilePath' ], 'w' ) aTimer = DebouncingTimer ( 1 , aTaskName , aTask , taskLog , signal . SIGHUP ) debouncingTimers . append ( aTimer ) # add watches asyncio . create_task ( aWatcher . managePathsToWatchQueue ()) for aWatch in aTask [ 'watch' ] : await aWatcher . watchARootPath ( aWatch ) # Ensure the task is run at least once logger . debug ( \"First run of taskRunner for {} \" . format ( aTaskName )) await aTimer . reStart () # watch and run cmd if 'runOnce' not in aTask : async for event in aWatcher . watchForFileSystemEvents () : logger . debug ( \"File system event mask {} for file [ {} ] for task {} \" . format ( getMaskName ( event . mask ), event . name , aTaskName )) await aTimer . reStart ()","title":"cpawd.taskRunner"},{"location":"API/taskRunner/#cpawdtaskrunner","text":"This cpawd.taskRunner module implements the running of all watch-do tasks. The following description is illustrated in the interaction diagram below. The top level runTasks method initiates an asyncio.Tasks running the watchDo method for each watch-do task. The watchDo method reStart s an asyncio.Task , taskRunner , via a call to asyncio.ensure_future , to manage a (potentially long running) OS process. The watchDo task listens, in an FSWatcher.watchForFileSystemEvents loop, for any file system changes which might be happening, reStart ing the taskRunner task on any such changes. The taskRunner task starts by sleeping, during which time the taskRunner task can be cancelled and reStart ed (by the watchDo task). This short timeout period of cancel-able sleep acts as a debouncing timer. It allows the watchDo task to frequently reStart the taskRunner task without actually running the external OS process until any nearly simultaneous file system changes have stopped. If the taskRunner is not cancelled during the sleep, the taskRunner starts the OS process, using a call to asyncio.create_subprocess_exec , and then creates two further asyncio.Tasks , captureStdout and captureRetCode , to manage the process's output as well as to wait for the process's return code. Once an external OS process has been started, any reStart requests from the watchDo task, signals the captureStdout task to stop listening for the process's stdout, and then sends a SIGHUP signal to the process (which must respond by gracefully exiting). The watchDo task then wait s on the taskRunner task to finish before creating a new taskRunner task (and potentially repeating this cycle). The main cpawd task, can at any time request that the runTasks task shutdown. To shutdown, the runTasks task first signals all of the watchDo FSWatcher.watchForFileSystemEvents loops to stop watching for file system events. Then the runTasks task signals all running taskRunners to stop. In this interaction diagram, each asyncio.Task is represented by the function which the task runs. The OSproc thread is an external OS process, which is the ultimate \"task\" of a given watch-do task. sequenceDiagram participant main participant runTasks participant watchDo participant taskRunner participant captureRetCode participant captureStdout participant OSproc activate main main-->>runTasks: run activate runTasks runTasks-->>watchDo: create_task activate watchDo Note over watchDo,OSproc: running (one watchDo for each watch-do task) ... watchDo-->>watchDo: reStart watchDo-->>watchDo: stopTaskProc Note over watchDo,OSproc: no task/proc running so stopTaskProc does nothing watchDo-->>taskRunner: ensure_future activate taskRunner taskRunner-->>taskRunner: sleep Note over taskRunner: a watchDo reStart<br/>at this point<br/>can cancel<br/>taskRunner<br/>while sleeping taskRunner-->>OSproc: exec activate OSproc taskRunner-->>captureStdout: wait on created task activate captureStdout taskRunner-->>captureRetCode: wait on created task activate captureRetCode OSproc-->>captureStdout: stdout OSproc-->>captureStdout: stdout OSproc-->>captureStdout: stdout Note over watchDo: file system<br/>change detected watchDo-->>watchDo: reStart watchDo-->>watchDo: stopTaskProc watchDo-->>OSproc: send SIGHUP OSproc-->>captureRetCode: finished deactivate OSproc captureRetCode-->>taskRunner: finished deactivate captureRetCode watchDo-->>captureStdout: continueCapturingStdout = False captureStdout-->>taskRunner: finished deactivate captureStdout watchDo-->>taskRunner: wait taskRunner-->>watchDo: done deactivate taskRunner watchDo-->>taskRunner: ensure_future activate taskRunner Note over taskRunner,OSproc: new taskRunner starts ... taskRunner-->>watchDo: done deactivate taskRunner main-->>runTasks: shutdown runTasks-->>watchDo: stop listening for<br/>file system events runTasks-->>watchDo: stop debouncing timers watchDo-->>watchDo: stopTaskProc watchDo-->>runTasks: done deactivate watchDo runTasks-->>main: done deactivate runTasks deactivate main","title":"cpawd.taskRunner"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer","text":"The DebouncingTimer class implements a simple timer to ensure multiple file system events result in only one invocation of the task command.","title":"DebouncingTimer"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.__init__","text":"Create the timer with a specific timeout and task definition. The taskDetails provides the command to run, the log file used to record command output, as well as the project directory in which to run the command. Source code in cpawd/taskRunner.py def __init__ ( self , timeout , taskName , taskDetails , taskLog , terminateSignal ) : \"\"\" Create the timer with a specific timeout and task definition. The taskDetails provides the command to run, the log file used to record command output, as well as the project directory in which to run the command. \"\"\" self . timeout = timeout self . taskName = taskName self . taskCmd = taskDetails [ 'cmd' ] self . taskCmdStr = \" \" . join ( taskDetails [ 'cmd' ]) self . taskLog = taskLog self . taskDir = taskDetails [ 'projectDir' ] self . termSignal = terminateSignal self . taskFuture = None self . proc = None self . pid = None self . retCode = None self . continueCapturingStdout = True","title":"__init__()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.cancelTimer","text":"Cancel the Debouncing timer Source code in cpawd/taskRunner.py def cancelTimer ( self ) : \"\"\"Cancel the Debouncing timer\"\"\" if self . taskFuture and not self . procIsRunning () : logger . debug ( \"Cancelling timer for {} \" . format ( self . taskName )) self . taskFuture . cancel ()","title":"cancelTimer()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.captureOutput","text":"Capture the (stdout) output from the external process Source code in cpawd/taskRunner.py async def captureOutput ( self ) : \"\"\"Capture the (stdout) output from the external process\"\"\" logger . debug ( \"CaptureOutput task running for {} \" . format ( self . taskName )) taskLog = self . taskLog if self . proc is not None : stdout = self . proc . stdout if stdout : await taskLog . write ( \" \\n ============================================================================ \\n \" ) await taskLog . write ( \" {} ( {} ) stdout @ {} \\n \" . format ( self . taskName , self . proc . pid , time . strftime ( \"%Y/%m/ %d %H:%M:%S\" ) )) await taskLog . write ( \" {} \\n \" . format ( self . taskCmdStr )) await taskLog . write ( \"---------------------------------------------------------------------------- \\n \" ) await taskLog . flush () while self . continueCapturingStdout and not stdout . at_eof () : logger . debug ( \"Collecting {} stdout ( {} )\" . format ( self . taskName , self . proc . pid )) aLine = await stdout . readline () await taskLog . write ( aLine . decode ()) await taskLog . flush () if self . continueCapturingStdout : logger . debug ( \"Finshed collecting {} stdout ( {} )\" . format ( self . taskName , self . proc . pid )) else : await taskLog . write ( \" \\n [Stopped collecting stdout]\" ) logger . debug ( \"Stopped collecting process stdout for {} ( {} )\" . format ( self . taskName , self . pid )) await taskLog . write ( \" \\n ---------------------------------------------------------------------------- \\n \" ) await taskLog . write ( \" {} ( {} ) stdout @ {} \\n \" . format ( self . taskName , self . pid , time . strftime ( \"%Y/%m/ %d %H:%M:%S\" ) )) await taskLog . flush () else : logger . debug ( \"No stdout found for {} \" . format ( self . taskName )) else : logger . debug ( \"No external process found so no stdout captured for {} \" . format ( self . taskName )) logger . debug ( \"CaptureOutput task finished for {} \" . format ( self . taskName ))","title":"captureOutput()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.captureRetCode","text":"Wait for and capture the return code of the external process Source code in cpawd/taskRunner.py async def captureRetCode ( self ) : \"\"\"Wait for and capture the return code of the external process\"\"\" logger . debug ( \"Capturing return code for {} \" . format ( self . taskName )) try : self . retCode = await self . proc . wait () except ProcessLookupError : logger . debug ( \"No process found for {} (pid: {} )\" . format ( self . taskName , self . pid )) if self . retCode is not None : retCode = self . retCode pid = self . pid logger . debug ( \"Return code for {} is {} (pid: {} )\" . format ( self . taskName , retCode , pid )) taskLog = self . taskLog await taskLog . write ( \" {} task ( {} ) exited with {} \\n \" . format ( self . taskName , pid , retCode )) await taskLog . write ( \" \\n \" ) await taskLog . flush () logger . debug ( \"Finished {} ( {} ) command [ {} ] exited with {} \" . format ( self . taskName , pid , self . taskCmdStr , retCode )) self . proc = None logger . debug ( \"Captured return code for {} \" . format ( self . taskName ))","title":"captureRetCode()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.procIsRunning","text":"Determine if an external process is (still) running Source code in cpawd/taskRunner.py def procIsRunning ( self ) : \"\"\"Determine if an external process is (still) running\"\"\" return self . proc is not None and self . proc . returncode is None","title":"procIsRunning()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.reStart","text":"(Re)Start the timer. If the timer is already started, it is restarted with a new timeout period. Source code in cpawd/taskRunner.py async def reStart ( self ) : \"\"\" (Re)Start the timer. If the timer is already started, it is restarted with a new timeout period. \"\"\" await self . stopTaskProc () if self . taskFuture : self . cancelTimer () if not self . taskFuture . done () : logger . debug ( \"Waiting for the previous taskRunner task for {} to finish\" . format ( self . taskName )) await asyncio . wait ([ self . taskFuture ]) logger . debug ( \"Starting new taskRunner for {} \" . format ( self . taskName )) self . taskFuture = asyncio . ensure_future ( self . taskRunner ())","title":"reStart()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.stopTaskProc","text":"Stop the external process Source code in cpawd/taskRunner.py async def stopTaskProc ( self ) : \"\"\"Stop the external process\"\"\" logger . debug ( \"Attempting to stop the task process for {} \" . format ( self . taskName )) self . continueCapturingStdout = False if self . proc is not None : pid = self . proc . pid logger . debug ( \"Process found for {} ( {} )\" . format ( self . taskName , pid )) if self . procIsRunning () : logger . debug ( \"Process still running for {} \" . format ( self . taskName )) try : logger . debug ( \"Sending OS signal ( {} ) to {} (pid: {} )\" . format ( self . termSignal , self . taskName , pid )) self . proc . send_signal ( self . termSignal ) except ProcessLookupError : logger . debug ( \"No exiting external process found for {} (pid: {} )\" . format ( self . taskName , pid )) except Exception as err : logger . error ( \"Could not send signal ( {} ) to proc for {} (})\" . format ( self . termSignal , self . taskName , pid )) logger . error ( repr ( err )) traceback . print_exc () else : self . retCode = self . proc . returncode logger . debug ( \"Process finished with return code {} for {} \" . format ( self . retCode , self . taskName )) else : logger . debug ( \"No external process found for {} \" . format ( self . taskName ))","title":"stopTaskProc()"},{"location":"API/taskRunner/#cpawd.taskRunner.DebouncingTimer.taskRunner","text":"Run the task's command, after sleeping for the timeout period, using asyncio.create_subprocess_exec command. Source code in cpawd/taskRunner.py async def taskRunner ( self ) : \"\"\" Run the task's command, after sleeping for the timeout period, using `asyncio.create_subprocess_exec` command. \"\"\" try : logger . debug ( \"TaskRunner for {} sleeping for {} \" . format ( self . taskName , self . timeout )) await asyncio . sleep ( self . timeout ) # Now we can run the new task... # logger . debug ( \"Running {} command [ {} ]\" . format ( self . taskName , self . taskCmdStr )) self . proc = await asyncio . create_subprocess_exec ( * self . taskCmd , stdout = asyncio . subprocess . PIPE , stderr = asyncio . subprocess . STDOUT , cwd = self . taskDir ) self . pid = self . proc . pid self . retCode = None self . continueCapturingStdout = True print ( f 'Ran: { self . taskName } ' ) #await asyncio.gather( await self . captureOutput (), await self . captureRetCode () #) if self . retCode is None or self . retCode != 0 : print ( f \"FAILED: { self . taskName } ( { self . retCode } )\" ) except Exception as err : print ( \"Caught exception while running {} task\" . format ( self . taskName )) print ( repr ( err )) traceback . print_exc ()","title":"taskRunner()"},{"location":"API/taskRunner/#cpawd.taskRunner.runTasks","text":"Walk through the list of watch-do tasks and create an asyncio.Task for each task, using an invocation of the watchDo method to wrap each task. Since these tasks are not Python-CPU bound, they will essentially \"run\" in parallel. Source code in cpawd/taskRunner.py async def runTasks ( config ) : \"\"\" Walk through the list of watch-do tasks and create an `asyncio.Task` for each task, using an invocation of the `watchDo` method to wrap each task. Since these tasks are not Python-CPU bound, they will essentially \"run\" in parallel. \"\"\" for aTaskName , aTask in config [ 'tasks' ] . items () : asyncio . create_task ( watchDo ( aTaskName , aTask )) await waitForShutdown ()","title":"runTasks()"},{"location":"API/taskRunner/#cpawd.taskRunner.stopTasks","text":"Stop all watch-do tasks Source code in cpawd/taskRunner.py async def stopTasks () : \"\"\"Stop all watch-do tasks\"\"\" logger . info ( \"Stopping all tasks\" ) for aWatcher in watchers : aWatcher . stopWatchingFileSystem () for aTimer in debouncingTimers : await aTimer . stopTaskProc () aTimer . cancelTimer () logger . debug ( \"All tasks Stoped\" )","title":"stopTasks()"},{"location":"API/taskRunner/#cpawd.taskRunner.waitForShutdown","text":"Wait for the shutdown event and then stop all watch-do tasks Source code in cpawd/taskRunner.py async def waitForShutdown () : \"\"\"Wait for the shutdown event and then stop all watch-do tasks\"\"\" logger . debug ( \"waiting for eventual shutdown event\" ) await shutdownTasks . wait () logger . debug ( \"got shutdown\" ) await stopTasks () logger . debug ( \"shutdown\" )","title":"waitForShutdown()"},{"location":"API/taskRunner/#cpawd.taskRunner.watchDo","text":"Setup and manage the watches, and then run the task's command using the DebouncingTimer whenever a change is detected in a watched directory or file. Source code in cpawd/taskRunner.py async def watchDo ( aTaskName , aTask ) : \"\"\" Setup and manage the watches, and then run the task's command using the DebouncingTimer whenever a change is detected in a watched directory or file. \"\"\" logger . debug ( \"Starting watchDo for {} \" . format ( aTaskName )) aWatcher = FSWatcher ( logger ) watchers . append ( aWatcher ) taskLog = await aiofiles . open ( aTask [ 'logFilePath' ], 'w' ) aTimer = DebouncingTimer ( 1 , aTaskName , aTask , taskLog , signal . SIGHUP ) debouncingTimers . append ( aTimer ) # add watches asyncio . create_task ( aWatcher . managePathsToWatchQueue ()) for aWatch in aTask [ 'watch' ] : await aWatcher . watchARootPath ( aWatch ) # Ensure the task is run at least once logger . debug ( \"First run of taskRunner for {} \" . format ( aTaskName )) await aTimer . reStart () # watch and run cmd if 'runOnce' not in aTask : async for event in aWatcher . watchForFileSystemEvents () : logger . debug ( \"File system event mask {} for file [ {} ] for task {} \" . format ( getMaskName ( event . mask ), event . name , aTaskName )) await aTimer . reStart ()","title":"watchDo()"}]}